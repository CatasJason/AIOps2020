{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\trace_csf.csv',\n",
       " 'C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\trace_fly_remote.csv',\n",
       " 'C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\trace_jdbc.csv',\n",
       " 'C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\trace_local.csv',\n",
       " 'C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\trace_osb.csv',\n",
       " 'C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\trace_remote_process.csv']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\spkgy\\\\OneDrive\\\\Documents\\\\Tsinghua\\\\Advanced Network Management\\\\Group Project\\\\AIOps挑战赛数据\\\\AIOps挑战赛数据\\\\2020_05_22\\\\2020_05_22\\\\调用链指标\\\\\"\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    if df['callType'].iloc[0]=='JDBC' or df['callType'].iloc[0]=='LOCAL':\n",
    "        df['serviceName'] = df['dsName']\n",
    "    elif df['callType'].iloc[0]=='RemoteProcess' or df['callType'].iloc[0]=='OSB':\n",
    "        df['serviceName'] = df['cmdb_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('dsName', axis=1)\n",
    "# df = df.drop(['path'], axis=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_time = 1590353220000\n",
    "df = df[(df.startTime >= (anomaly_time-1200000)) & (df.startTime <= (anomaly_time + 300000))]\n",
    "df.to_csv(path + 'trace_data_os18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['callType']=='RemoteProcess']\n",
    "df1 = df1[['pid','cmdb_id']]\n",
    "df1 = df1.set_index('pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_cmdb = df1.to_dict()\n",
    "csf_cmdb = {str(key):str(values) for key, values in csf_cmdb['cmdb_id'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    if row['id'] in csf_cmdb:\n",
    "        df.at[index, 'serviceName'] = csf_cmdb[row['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapse_time = {}\n",
    "children = {}\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    if row['pid'] != 'None':\n",
    "        if row['pid'] in children.keys():\n",
    "            children[row['pid']].append(row['id'])\n",
    "        else:\n",
    "            children[row['pid']] = [row['id']]\n",
    "    elapse_time[row['id']] = float(row['elapsedTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actual_time'] = 0.0\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    total_child = 0.0\n",
    "    if row['id'] not in children.keys():\n",
    "        df.at[index, 'actual_time'] = row['elapsedTime']\n",
    "        continue\n",
    "    for child in children[row['id']]:\n",
    "        total_child += elapse_time[child]\n",
    "    df.at[index, 'actual_time'] = row['elapsedTime'] - total_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['traceId','startTime','callType'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.serviceName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['cmdb_id', 'serviceName'])[['startTime','actual_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esd_test_statistics(x, hybrid=True):\n",
    "  \"\"\"\n",
    "  Compute the location and dispersion sample statistics used to carry out the ESD test.\n",
    "  \"\"\"\n",
    "  if hybrid:\n",
    "    location = pd.Series(x).median(skipna=True) # Median\n",
    "    dispersion = np.median(np.abs(x - np.median(x))) # Median Absolute Deviation\n",
    "  else:  \n",
    "    location = pd.Series(x).mean(skipna=True) # Mean\n",
    "    dispersion = pd.Series(x).std(skipna=True) # Standard Deviation\n",
    "    \n",
    "  return location, dispersion    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seasonal_mean(x, freq):\n",
    "#   \"\"\"\n",
    "#   Returns the mean of the timeseries for each period in x.\n",
    "  \n",
    "#   x    : List, array, or series containing the time series\n",
    "#   freq : Int that gives the number of periods per cycle (7 for week, 12 for monthly, etc)\n",
    "#   \"\"\"\n",
    "#   return np.array([pd.Series(x[i::freq]).mean(skipna=True) for i in range(freq)])\n",
    "\n",
    "def ts_S_Md_decomposition(x):\n",
    "  \"\"\"\n",
    "  Decomposes the timeseries using a modified STL method:Rx = X - Sx - X^~.\n",
    "    Rx : Residuals\n",
    "    X  : Original time series\n",
    "    Sx : Seasonality component (average value per period)\n",
    "    X^~: Median of original timeseries\n",
    "    \n",
    "  x    : List, array, or series containing the time series \n",
    "  freq : Int that gives the number of periods per cycle (7 for week, 12 for monthly, etc)  \n",
    "  \"\"\"\n",
    "  nobs = len(x)\n",
    "  \n",
    "  # Seasonality\n",
    "#   period_averages = seasonal_mean(x, freq)\n",
    "#   seasonal = np.tile(period_averages, nobs // freq + 1)[:nobs]\n",
    "  \n",
    "  # Median\n",
    "  med = np.tile(pd.Series(x).median(skipna=True), nobs)\n",
    "  \n",
    "  # Residuals\n",
    "#   res = np.array(x) - seasonal - med\n",
    "  res = np.array(x) - med\n",
    "  return {\"observed\": np.array(x), \"median\":med, \"residual\":res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esd_test(x, alpha=0.95, ub=0.499, hybrid=True):\n",
    "  \"\"\"\n",
    "  Carries out the Extreme Studentized Deviate(ESD) test which can be used to detect one or more outliers present in the timeseries\n",
    "  \n",
    "  x      : List, array, or series containing the time series\n",
    "  freq   : Int that gives the number of periods per cycle (7 for week, 12 for monthly, etc)\n",
    "  alpha  : Confidence level in detecting outliers\n",
    "  ub     : Upper bound on the fraction of datapoints which can be labeled as outliers (<=0.499)\n",
    "  hybrid : Whether to use the robust statistics (median, median absolute error) or the non-robust versions (mean, standard deviation) to test for anomalies\n",
    "  \"\"\"\n",
    "  nobs = len(x)\n",
    "  if ub > 0.4999:\n",
    "    ub = 0.499\n",
    "  k = max(int(np.floor(ub * nobs)), 1) # Maximum number of anomalies. At least 1 anomaly must be tested.\n",
    "#   res_tmp = ts_S_Md_decomposition(x)[\"residual\"] # Residuals from time series decomposition\n",
    "    \n",
    "  # Carry out the esd test k times  \n",
    "  res = np.ma.array(x, mask=False) # The \"ma\" structure allows masking of values to exclude the elements from any calculation\n",
    "  anomalies = [] # returns the indices of the found anomalies\n",
    "  for i in range(1, k+1):\n",
    "    location, dispersion = esd_test_statistics(res, hybrid) # Sample statistics\n",
    "    tmp = np.abs(res - location) / dispersion\n",
    "    idx = np.argmax(tmp) # Index of the test statistic\n",
    "    test_statistic = tmp[idx] \n",
    "    n = nobs - res.mask.sum() # sums  nonmasked values\n",
    "    critical_value = (n - i) * t.ppf(alpha, n - i - 1) / np.sqrt((n - i - 1 + np.power(t.ppf(alpha, n - i - 1), 2)) * (n - i - 1)) \n",
    "    if test_statistic > critical_value:\n",
    "      anomalies.append(test_statistic)\n",
    "    res.mask[idx] = True  \n",
    "  return np.mean(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCA():\n",
    "    def local_initiate(self):\n",
    "        self.dockers = ['docker_001', 'docker_002', 'docker_003', 'docker_004',\n",
    "                'docker_005', 'docker_006', 'docker_007', 'docker_008']\n",
    "        self.docker_hosts = ['os_017', 'os_018', 'os_019', 'os_020']\n",
    "\n",
    "        self.docker_kpi_names = ['container_cpu_used', None]\n",
    "        self.os_kpi_names = ['Sent_queue', 'Received_queue']\n",
    "        self.db_kpi_names = ['Proc_User_Used_Pct','Proc_Used_Pct','Sess_Connect','On_Off_State', 'tnsping_result_time']\n",
    "\n",
    "        self.docker_lookup_table = {}\n",
    "        for i in range(len(self.dockers)):\n",
    "            self.docker_lookup_table[self.dockers[i]] = self.docker_hosts[i % 4]\n",
    "            \n",
    "    def find_anomalous_rows(self, min_threshold = 5):\n",
    "        table = self.anomaly_chart.copy()\n",
    "        threshold = max( 0.5 * table.stack().max(), min_threshold)\n",
    "        dodgy_rows = []\n",
    "        just_rows = []\n",
    "        for column in table:\n",
    "            v = 0\n",
    "            r = ''\n",
    "            for index, row in table.iterrows():\n",
    "                if (row[column] > threshold):\n",
    "                    if index == column:\n",
    "                        dodgy_rows.append([index, row[column]])\n",
    "                        just_rows.append(index)\n",
    "                        break\n",
    "                    elif (row[column] > v):\n",
    "                        v = row[column]\n",
    "                        r = index\n",
    "            if r != '':\n",
    "                dodgy_rows.append([r, column, v])\n",
    "                just_rows.append(r)\n",
    "        \n",
    "        output = self.localize(dodgy_rows, list(set(just_rows)))\n",
    "        return output\n",
    "\n",
    "\n",
    "    def find_anomalous_kpi(self, cmdb_id):\n",
    "        kpi_names = []\n",
    "        if 'os' in cmdb_id:\n",
    "            kpi_names = self.os_kpi_names\n",
    "        elif 'docker' in cmdb_id:\n",
    "            kpi_names = self.docker_kpi_names\n",
    "        else:\n",
    "            kpi_names = self.db_kpi_names\n",
    "\n",
    "        return kpi_names\n",
    "\n",
    "\n",
    "    def localize(self, dodgy_rows, just_rows):\n",
    "        n = len(just_rows)\n",
    "        if n < 1:\n",
    "            return None\n",
    "        if n == 1:\n",
    "            KPIs = self.find_anomalous_kpi(just_rows[0])\n",
    "            to_be_sent = []\n",
    "            for KPI in KPIs:\n",
    "                to_be_sent.append([just_rows[0], KPI])\n",
    "            return to_be_sent\n",
    "        if n == 2:\n",
    "            r0 = just_rows[0]\n",
    "            r1 = just_rows[1]\n",
    "            if ('os' in r0) and ('os' in r1):\n",
    "                KPI = self.find_anomalous_kpi('os_001')\n",
    "                return [['os_001', KPI[0]],['os_001', KPI[1]]]\n",
    "            elif ('docker' in r0) and ('docker' in r1):\n",
    "                if self.docker_lookup_table[r0] == self.docker_lookup_table[r1]:\n",
    "                    KPI = self.find_anomalous_kpi(self.docker_lookup_table[r0])\n",
    "                    return [[self.docker_lookup_table[r0], KPI]]\n",
    "            else:\n",
    "                KPI0s = self.find_anomalous_kpi(r0)\n",
    "                KPI1s = self.find_anomalous_kpi(r1)\n",
    "                to_be_sent = []\n",
    "                for kpi in KPI0s:\n",
    "                    to_be_sent.append([r0, kpi])\n",
    "                for kpi in KPI1s:\n",
    "                    to_be_sent.append([r1, kpi])\n",
    "                return to_be_sent\n",
    "        if n > 2:\n",
    "            dodgy_rows.sort(key = lambda x: x[2], reverse = True)\n",
    "            just_rows = [x[0] for x in dodgy_rows]\n",
    "            just_rows = list(set(just_rows))\n",
    "            return self.localize(dodgy_rows[:2], just_rows[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca = RCA()\n",
    "rca.local_initiate()\n",
    "rca.anomaly_chart = anomaly_chart\n",
    "output = rca.find_anomalous_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_chart = pd.DataFrame()\n",
    "for (a, b), value in grouped_df:\n",
    "    value['time_group'] = value.startTime//60000\n",
    "    value =value.groupby(['time_group'])['actual_time'].mean().reset_index()\n",
    "    result = esd_test(value['actual_time'].to_numpy(), alpha=0.95, ub=0.02, hybrid=True)\n",
    "    anomaly_chart.loc[b,a] = result\n",
    "    # print(a,b,' = ',len(result)/len(value)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_chart.sort_index(inplace=True)\n",
    "anomaly_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = grouped_df.get_group(('docker_008', 'docker_008'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test[['startTime','actual_time']]\n",
    "# x_val = x['actual_time'].to_numpy()\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['time_group'] = x.startTime//10000\n",
    "x = x.groupby(['time_group'])['actual_time'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x['actual_time'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = esd_test(x_val, freq=3, alpha=0.99, ub=0.4, hybrid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pd.Series(x_val).index, x_val, color=\"blue\", label = \"Original\")\n",
    "ax.scatter(result, x_val[result], color='red', label='Anomaly')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test[['startTime','actual_time']]\n",
    "# x_val = x['actual_time'].to_numpy()\n",
    "# x\n",
    "x['time_group'] = x.startTime//1000\n",
    "x = x.groupby(['time_group'])['actual_time'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x.copy()\n",
    "x_test.index = pd.to_datetime(x_test['time_group'], unit=\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}