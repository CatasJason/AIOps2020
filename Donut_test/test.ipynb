{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361264bitdonutcondad95a3912f57c46d7a60540e6fae9691f",
   "display_name": "Python 3.6.12 64-bit ('donut': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "from tfsnippet.utils import get_variables_as_dict, VariableSaver\n",
    "from donut import complete_timestamp, standardize_kpi\n",
    "from donut import Donut"
   ]
  },
  {
   "source": [
    "# Import and pre-process data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data.\n",
    "df = pd.read_csv(\"data/g.csv\")\n",
    "timestamp, values, labels = df[\"timestamp\"],df[\"value\"], df[\"label\"]\n",
    "# If there is no label, simply use all zeros.\n",
    "labels = np.zeros_like(values, dtype=np.int32)\n",
    "\n",
    "# Complete the timestamp, and obtain the missing point indicators.\n",
    "timestamp, missing, (values, labels) = complete_timestamp(timestamp, (values, labels))\n",
    "\n",
    "# Split the training and testing data.\n",
    "test_portion = 0.25\n",
    "test_n = int(len(values) * test_portion)\n",
    "train_values, test_values = values[:-test_n], values[-test_n:]\n",
    "train_labels, test_labels = labels[:-test_n], labels[-test_n:]\n",
    "train_missing, test_missing = missing[:-test_n], missing[-test_n:]\n",
    "\n",
    "# Standardize the training and testing data.\n",
    "train_values, mean, std = standardize_kpi(\n",
    "    train_values, excludes=np.logical_or(train_labels, train_missing))\n",
    "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)"
   ]
  },
  {
   "source": [
    "# Create the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=120,\n",
    "        z_dims=5,\n",
    "    )"
   ]
  },
  {
   "source": [
    "# Training and Testing (+ Saving)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32.3666 (±6.89898); valid loss: 52.6482\n",
      "[Epoch 226/256, Step 99500, ETA 47s] step time: 0.003152s (±0.01017s); valid time: 0.1023s; loss: -32.4286 (±7.57531); valid loss: 55.8698\n",
      "[Epoch 226/256, Step 99600, ETA 46.66s] step time: 0.003148s (±0.01057s); valid time: 0.1062s; loss: -31.2287 (±8.26531); valid loss: 63.3405\n",
      "[Epoch 226/256, Step 99700, ETA 46.31s] step time: 0.003184s (±0.01046s); valid time: 0.1052s; loss: -32.4575 (±7.00791); valid loss: 55.605\n",
      "[Epoch 226/256, Step 99800, ETA 45.97s] step time: 0.003138s (±0.01031s); valid time: 0.1035s; loss: -31.7053 (±6.82148); valid loss: 57.4179\n",
      "[Epoch 227/256, Step 99900, ETA 45.63s] step time: 0.003193s (±0.01034s); valid time: 0.1039s; loss: -31.0612 (±7.77678); valid loss: 54.7747\n",
      "[Epoch 227/256, Step 100000, ETA 45.29s] step time: 0.003243s (±0.01051s); valid time: 0.1056s; loss: -31.9256 (±6.56529); valid loss: 52.6022\n",
      "[Epoch 227/256, Step 100100, ETA 44.94s] step time: 0.003204s (±0.01066s); valid time: 0.1071s; loss: -32.0013 (±6.51689); valid loss: 57.1363\n",
      "[Epoch 227/256, Step 100200, ETA 44.6s] step time: 0.003064s (±0.009953s); valid time: 0.1s; loss: -31.2912 (±7.44603); valid loss: 54.8504\n",
      "[Epoch 227/256, Step 100300, ETA 44.25s] step time: 0.003028s (±0.00969s); valid time: 0.09722s; loss: -31.6084 (±7.49812); valid loss: 49.9105\n",
      "[Epoch 228/256, Step 100400, ETA 43.91s] step time: 0.003016s (±0.01006s); valid time: 0.1011s; loss: -32.0267 (±7.40897); valid loss: 53.5235\n",
      "[Epoch 228/256, Step 100500, ETA 43.57s] step time: 0.00312s (±0.009985s); valid time: 0.1003s; loss: -31.7434 (±7.34606); valid loss: 52.9714\n",
      "[Epoch 228/256, Step 100600, ETA 43.22s] step time: 0.003112s (±0.01018s); valid time: 0.1022s; loss: -31.7968 (±8.30283); valid loss: 56.3247\n",
      "[Epoch 228/256, Step 100700, ETA 42.87s] step time: 0.003093s (±0.01036s); valid time: 0.104s; loss: -32.9339 (±6.88324); valid loss: 48.975\n",
      "[Epoch 229/256, Step 100800, ETA 42.54s] step time: 0.003031s (±0.009719s); valid time: 0.09775s; loss: -31.2754 (±7.0695); valid loss: 60.6972\n",
      "[Epoch 229/256, Step 100900, ETA 42.19s] step time: 0.003104s (±0.01048s); valid time: 0.1053s; loss: -31.612 (±7.12749); valid loss: 57.6523\n",
      "[Epoch 229/256, Step 101000, ETA 41.84s] step time: 0.003051s (±0.01025s); valid time: 0.103s; loss: -32.1535 (±7.04456); valid loss: 60.0405\n",
      "[Epoch 229/256, Step 101100, ETA 41.5s] step time: 0.003065s (±0.009852s); valid time: 0.09902s; loss: -32.2207 (±7.66455); valid loss: 57.6996\n",
      "[Epoch 229/256, Step 101200, ETA 41.15s] step time: 0.003052s (±0.01s); valid time: 0.1004s; loss: -31.187 (±8.12779); valid loss: 58.5593\n",
      "[Epoch 230/256, Step 101300, ETA 40.81s] step time: 0.002974s (±0.009582s); valid time: 0.09626s; loss: -30.931 (±7.39501); valid loss: 68.2184\n",
      "[Epoch 230/256, Step 101400, ETA 40.46s] step time: 0.003011s (±0.009942s); valid time: 0.0998s; loss: -31.1301 (±6.69212); valid loss: 53.5087\n",
      "[Epoch 230/256, Step 101500, ETA 40.12s] step time: 0.003023s (±0.009814s); valid time: 0.09859s; loss: -32.7478 (±7.56394); valid loss: 60.4002\n",
      "[Epoch 230/256, Step 101600, ETA 39.77s] step time: 0.003084s (±0.01004s); valid time: 0.1009s; loss: -31.8934 (±7.63382); valid loss: 54.359\n",
      "[Epoch 230/256, Step 101660, ETA 39.56s] Learning rate decreased to 1.337855036737778e-06\n",
      "[Epoch 231/256, Step 101700, ETA 39.43s] step time: 0.003083s (±0.01014s); valid time: 0.1019s; loss: -31.6905 (±7.29393); valid loss: 50.2413\n",
      "[Epoch 231/256, Step 101800, ETA 39.09s] step time: 0.003201s (±0.01039s); valid time: 0.1045s; loss: -31.6897 (±6.86877); valid loss: 57.118\n",
      "[Epoch 231/256, Step 101900, ETA 38.74s] step time: 0.003176s (±0.01045s); valid time: 0.1051s; loss: -31.1503 (±7.85868); valid loss: 59.7243\n",
      "[Epoch 231/256, Step 102000, ETA 38.4s] step time: 0.003149s (±0.01069s); valid time: 0.1074s; loss: -31.3926 (±7.27148); valid loss: 60.9025\n",
      "[Epoch 231/256, Step 102100, ETA 38.05s] step time: 0.003183s (±0.01062s); valid time: 0.1067s; loss: -32.5157 (±7.83911); valid loss: 55.9914\n",
      "[Epoch 232/256, Step 102200, ETA 37.71s] step time: 0.003088s (±0.01031s); valid time: 0.1035s; loss: -32.3007 (±6.65224); valid loss: 53.2035\n",
      "[Epoch 232/256, Step 102300, ETA 37.37s] step time: 0.003137s (±0.0106s); valid time: 0.1063s; loss: -31.7067 (±7.79053); valid loss: 58.9982\n",
      "[Epoch 232/256, Step 102400, ETA 37.02s] step time: 0.003152s (±0.01038s); valid time: 0.1043s; loss: -31.7659 (±7.39299); valid loss: 51.8852\n",
      "[Epoch 232/256, Step 102500, ETA 36.68s] step time: 0.003202s (±0.01079s); valid time: 0.1082s; loss: -32.1043 (±7.61609); valid loss: 55.232\n",
      "[Epoch 233/256, Step 102600, ETA 36.34s] step time: 0.003173s (±0.01031s); valid time: 0.1037s; loss: -31.6476 (±6.53967); valid loss: 52.6679\n",
      "[Epoch 233/256, Step 102700, ETA 35.99s] step time: 0.002979s (±0.009748s); valid time: 0.09789s; loss: -32.1766 (±7.04265); valid loss: 55.652\n",
      "[Epoch 233/256, Step 102800, ETA 35.65s] step time: 0.003042s (±0.009933s); valid time: 0.09983s; loss: -31.8703 (±8.45945); valid loss: 56.9539\n",
      "[Epoch 233/256, Step 102900, ETA 35.3s] step time: 0.003155s (±0.01054s); valid time: 0.1059s; loss: -30.9149 (±6.66303); valid loss: 55.5059\n",
      "[Epoch 234/256, Step 103000, ETA 34.96s] step time: 0.003024s (±0.009623s); valid time: 0.09678s; loss: -31.2054 (±6.8779); valid loss: 57.311\n",
      "[Epoch 234/256, Step 103100, ETA 34.62s] step time: 0.003057s (±0.01029s); valid time: 0.1034s; loss: -30.7743 (±7.19554); valid loss: 47.9424\n",
      "[Epoch 234/256, Step 103200, ETA 34.27s] step time: 0.00301s (±0.009847s); valid time: 0.09887s; loss: -33.085 (±7.02081); valid loss: 61.5983\n",
      "[Epoch 234/256, Step 103300, ETA 33.92s] step time: 0.003004s (±0.009818s); valid time: 0.09864s; loss: -32.0361 (±7.77231); valid loss: 54.0087\n",
      "[Epoch 234/256, Step 103400, ETA 33.58s] step time: 0.003075s (±0.009828s); valid time: 0.09869s; loss: -31.6912 (±7.24334); valid loss: 56.6512\n",
      "[Epoch 235/256, Step 103500, ETA 33.24s] step time: 0.003019s (±0.009792s); valid time: 0.09827s; loss: -31.5662 (±7.37116); valid loss: 50.4049\n",
      "[Epoch 235/256, Step 103600, ETA 32.89s] step time: 0.002971s (±0.009683s); valid time: 0.09722s; loss: -30.9095 (±7.1983); valid loss: 65.4206\n",
      "[Epoch 235/256, Step 103700, ETA 32.54s] step time: 0.003052s (±0.01008s); valid time: 0.1009s; loss: -32.6511 (±7.12059); valid loss: 57.0673\n",
      "[Epoch 235/256, Step 103800, ETA 32.2s] step time: 0.003064s (±0.01006s); valid time: 0.1011s; loss: -32.4356 (±7.21856); valid loss: 60.483\n",
      "[Epoch 236/256, Step 103900, ETA 31.86s] step time: 0.003155s (±0.01049s); valid time: 0.1053s; loss: -31.4155 (±6.89417); valid loss: 60.8114\n",
      "[Epoch 236/256, Step 104000, ETA 31.51s] step time: 0.003154s (±0.01045s); valid time: 0.105s; loss: -31.9582 (±6.95489); valid loss: 58.025\n",
      "[Epoch 236/256, Step 104100, ETA 31.17s] step time: 0.003117s (±0.01002s); valid time: 0.1007s; loss: -30.9481 (±7.38665); valid loss: 54.6285\n",
      "[Epoch 236/256, Step 104200, ETA 30.82s] step time: 0.003176s (±0.01047s); valid time: 0.1053s; loss: -32.3831 (±7.79742); valid loss: 51.4856\n",
      "[Epoch 236/256, Step 104300, ETA 30.48s] step time: 0.003125s (±0.009823s); valid time: 0.09864s; loss: -31.7794 (±6.74695); valid loss: 53.8844\n",
      "[Epoch 237/256, Step 104400, ETA 30.14s] step time: 0.003213s (±0.01029s); valid time: 0.103s; loss: -32.343 (±6.56571); valid loss: 51.3478\n",
      "[Epoch 237/256, Step 104500, ETA 29.8s] step time: 0.003224s (±0.01068s); valid time: 0.1073s; loss: -31.3883 (±6.99614); valid loss: 48.1246\n",
      "[Epoch 237/256, Step 104600, ETA 29.45s] step time: 0.003226s (±0.01049s); valid time: 0.1051s; loss: -31.202 (±7.35309); valid loss: 47.4425\n",
      "[Epoch 237/256, Step 104700, ETA 29.11s] step time: 0.003267s (±0.01055s); valid time: 0.106s; loss: -31.489 (±7.94187); valid loss: 50.8866\n",
      "[Epoch 238/256, Step 104800, ETA 28.77s] step time: 0.003148s (±0.01021s); valid time: 0.1025s; loss: -32.282 (±7.96623); valid loss: 54.2558\n",
      "[Epoch 238/256, Step 104900, ETA 28.42s] step time: 0.003082s (±0.01011s); valid time: 0.1016s; loss: -32.0384 (±7.23972); valid loss: 56.812\n",
      "[Epoch 238/256, Step 105000, ETA 28.08s] step time: 0.003223s (±0.01096s); valid time: 0.1102s; loss: -32.285 (±6.79162); valid loss: 57.6087\n",
      "[Epoch 238/256, Step 105100, ETA 27.73s] step time: 0.003141s (±0.01025s); valid time: 0.1029s; loss: -31.1203 (±7.04746); valid loss: 46.9663\n",
      "[Epoch 239/256, Step 105200, ETA 27.39s] step time: 0.003173s (±0.01045s); valid time: 0.1051s; loss: -31.7441 (±7.62966); valid loss: 59.339\n",
      "[Epoch 239/256, Step 105300, ETA 27.05s] step time: 0.00309s (±0.009981s); valid time: 0.1003s; loss: -31.8773 (±7.46272); valid loss: 47.873\n",
      "[Epoch 239/256, Step 105400, ETA 26.7s] step time: 0.00299s (±0.009799s); valid time: 0.09839s; loss: -32.1656 (±7.66908); valid loss: 45.8961\n",
      "[Epoch 239/256, Step 105500, ETA 26.36s] step time: 0.003072s (±0.01006s); valid time: 0.101s; loss: -31.1801 (±7.39802); valid loss: 52.5487\n",
      "[Epoch 239/256, Step 105600, ETA 26.01s] step time: 0.003037s (±0.01008s); valid time: 0.1013s; loss: -32.1678 (±7.91429); valid loss: 50.5643\n",
      "[Epoch 240/256, Step 105700, ETA 25.67s] step time: 0.003268s (±0.009596s); valid time: 0.09662s; loss: -30.897 (±7.44193); valid loss: 62.417\n",
      "[Epoch 240/256, Step 105800, ETA 25.32s] step time: 0.003102s (±0.0103s); valid time: 0.1033s; loss: -31.8071 (±6.8897); valid loss: 51.26\n",
      "[Epoch 240/256, Step 105900, ETA 24.98s] step time: 0.00304s (±0.009827s); valid time: 0.09872s; loss: -32.5542 (±8.28157); valid loss: 49.2157\n",
      "[Epoch 240/256, Step 106000, ETA 24.63s] step time: 0.003141s (±0.0102s); valid time: 0.1021s; loss: -31.9672 (±7.45448); valid loss: 51.5392\n",
      "[Epoch 240/256, Step 106080, ETA 24.35s] Learning rate decreased to 1.0033912775533336e-06\n",
      "[Epoch 241/256, Step 106100, ETA 24.29s] step time: 0.003048s (±0.01s); valid time: 0.1002s; loss: -30.806 (±7.99147); valid loss: 53.4498\n",
      "[Epoch 241/256, Step 106200, ETA 23.95s] step time: 0.003023s (±0.01001s); valid time: 0.1006s; loss: -31.7713 (±7.28242); valid loss: 57.4135\n",
      "[Epoch 241/256, Step 106300, ETA 23.6s] step time: 0.003063s (±0.00994s); valid time: 0.09997s; loss: -31.6233 (±7.07031); valid loss: 52.4161\n",
      "[Epoch 241/256, Step 106400, ETA 23.26s] step time: 0.003074s (±0.01036s); valid time: 0.1041s; loss: -32.3427 (±7.17825); valid loss: 54.2563\n",
      "[Epoch 241/256, Step 106500, ETA 22.91s] step time: 0.003057s (±0.009956s); valid time: 0.1s; loss: -31.6042 (±6.32096); valid loss: 52.4175\n",
      "[Epoch 242/256, Step 106600, ETA 22.57s] step time: 0.003013s (±0.00974s); valid time: 0.09788s; loss: -31.4933 (±6.92391); valid loss: 49.9091\n",
      "[Epoch 242/256, Step 106700, ETA 22.22s] step time: 0.002995s (±0.009845s); valid time: 0.09896s; loss: -32.7074 (±7.26652); valid loss: 51.1789\n",
      "[Epoch 242/256, Step 106800, ETA 21.88s] step time: 0.002979s (±0.009829s); valid time: 0.09873s; loss: -31.5733 (±7.08963); valid loss: 56.1521\n",
      "[Epoch 242/256, Step 106900, ETA 21.53s] step time: 0.002978s (±0.009648s); valid time: 0.09691s; loss: -31.9049 (±6.58629); valid loss: 51.3542\n",
      "[Epoch 243/256, Step 107000, ETA 21.19s] step time: 0.003012s (±0.009751s); valid time: 0.09793s; loss: -31.2235 (±8.41437); valid loss: 50.4911\n",
      "[Epoch 243/256, Step 107100, ETA 20.84s] step time: 0.003114s (±0.01054s); valid time: 0.1059s; loss: -31.2027 (±8.17878); valid loss: 60.7487\n",
      "[Epoch 243/256, Step 107200, ETA 20.5s] step time: 0.003174s (±0.01061s); valid time: 0.1065s; loss: -32.6293 (±6.6509); valid loss: 50.634\n",
      "[Epoch 243/256, Step 107300, ETA 20.15s] step time: 0.003088s (±0.009876s); valid time: 0.09932s; loss: -31.6272 (±8.19827); valid loss: 58.8748\n",
      "[Epoch 243/256, Step 107400, ETA 19.81s] step time: 0.00302s (±0.009703s); valid time: 0.09752s; loss: -32.0624 (±7.05738); valid loss: 59.3411\n",
      "[Epoch 244/256, Step 107500, ETA 19.47s] step time: 0.003057s (±0.009774s); valid time: 0.09824s; loss: -32.4866 (±7.21695); valid loss: 57.5134\n",
      "[Epoch 244/256, Step 107600, ETA 19.12s] step time: 0.003112s (±0.01015s); valid time: 0.1019s; loss: -31.3803 (±6.34419); valid loss: 57.4947\n",
      "[Epoch 244/256, Step 107700, ETA 18.78s] step time: 0.003277s (±0.01054s); valid time: 0.1059s; loss: -31.3108 (±6.70211); valid loss: 51.156\n",
      "[Epoch 244/256, Step 107800, ETA 18.43s] step time: 0.003188s (±0.01038s); valid time: 0.1042s; loss: -31.6018 (±8.52624); valid loss: 60.8252\n",
      "[Epoch 245/256, Step 107900, ETA 18.09s] step time: 0.003044s (±0.009995s); valid time: 0.1003s; loss: -31.7026 (±7.14416); valid loss: 53.4209\n",
      "[Epoch 245/256, Step 108000, ETA 17.75s] step time: 0.003144s (±0.01041s); valid time: 0.1047s; loss: -31.8918 (±7.35916); valid loss: 53.9439\n",
      "[Epoch 245/256, Step 108100, ETA 17.4s] step time: 0.003139s (±0.009819s); valid time: 0.09868s; loss: -32.2638 (±7.44554); valid loss: 52.4236\n",
      "[Epoch 245/256, Step 108200, ETA 17.05s] step time: 0.003009s (±0.009917s); valid time: 0.0995s; loss: -31.6054 (±6.81121); valid loss: 61.0882\n",
      "[Epoch 246/256, Step 108300, ETA 16.71s] step time: 0.003404s (±0.01033s); valid time: 0.1039s; loss: -32.7756 (±7.77573); valid loss: 52.0007\n",
      "[Epoch 246/256, Step 108400, ETA 16.37s] step time: 0.003014s (±0.009908s); valid time: 0.09952s; loss: -31.9135 (±7.42548); valid loss: 58.3036\n",
      "[Epoch 246/256, Step 108500, ETA 16.02s] step time: 0.003034s (±0.009868s); valid time: 0.09915s; loss: -32.9366 (±6.66744); valid loss: 58.8439\n",
      "[Epoch 246/256, Step 108600, ETA 15.68s] step time: 0.003034s (±0.009871s); valid time: 0.09914s; loss: -30.7865 (±7.92102); valid loss: 47.8607\n",
      "[Epoch 246/256, Step 108700, ETA 15.33s] step time: 0.002996s (±0.009802s); valid time: 0.09843s; loss: -32.1677 (±7.72313); valid loss: 60.9268\n",
      "[Epoch 247/256, Step 108800, ETA 14.99s] step time: 0.002993s (±0.009757s); valid time: 0.09796s; loss: -31.1121 (±6.55807); valid loss: 57.0126\n",
      "[Epoch 247/256, Step 108900, ETA 14.64s] step time: 0.003048s (±0.01021s); valid time: 0.1025s; loss: -32.3658 (±8.20444); valid loss: 55.0617\n",
      "[Epoch 247/256, Step 109000, ETA 14.3s] step time: 0.003116s (±0.009984s); valid time: 0.1003s; loss: -31.2922 (±6.56089); valid loss: 53.2662\n",
      "[Epoch 247/256, Step 109100, ETA 13.95s] step time: 0.003066s (±0.01024s); valid time: 0.1028s; loss: -31.9084 (±7.53694); valid loss: 52.3055\n",
      "[Epoch 248/256, Step 109200, ETA 13.61s] step time: 0.003314s (±0.01062s); valid time: 0.1063s; loss: -31.4569 (±6.60472); valid loss: 55.7988\n",
      "[Epoch 248/256, Step 109300, ETA 13.27s] step time: 0.003272s (±0.01043s); valid time: 0.1046s; loss: -31.5821 (±6.61206); valid loss: 53.7518\n",
      "[Epoch 248/256, Step 109400, ETA 12.93s] step time: 0.003343s (±0.01085s); valid time: 0.1089s; loss: -31.4186 (±7.67183); valid loss: 57.6018\n",
      "[Epoch 248/256, Step 109500, ETA 12.58s] step time: 0.003334s (±0.01116s); valid time: 0.1121s; loss: -31.9421 (±7.71955); valid loss: 55.7359\n",
      "[Epoch 248/256, Step 109600, ETA 12.24s] step time: 0.003241s (±0.0104s); valid time: 0.1043s; loss: -31.5867 (±7.00341); valid loss: 55.7046\n",
      "[Epoch 249/256, Step 109700, ETA 11.89s] step time: 0.003269s (±0.01089s); valid time: 0.1091s; loss: -31.7321 (±7.58181); valid loss: 58.0747\n",
      "[Epoch 249/256, Step 109800, ETA 11.55s] step time: 0.003252s (±0.01088s); valid time: 0.1092s; loss: -30.9014 (±7.50266); valid loss: 60.2489\n",
      "[Epoch 249/256, Step 109900, ETA 11.21s] step time: 0.003115s (±0.01046s); valid time: 0.1051s; loss: -31.6137 (±7.05841); valid loss: 52.437\n",
      "[Epoch 249/256, Step 110000, ETA 10.86s] step time: 0.003213s (±0.01039s); valid time: 0.1044s; loss: -31.7577 (±7.30096); valid loss: 52.2948\n",
      "[Epoch 250/256, Step 110100, ETA 10.52s] step time: 0.003035s (±0.009796s); valid time: 0.09847s; loss: -32.6728 (±7.64428); valid loss: 55.6262\n",
      "[Epoch 250/256, Step 110200, ETA 10.17s] step time: 0.002944s (±0.009448s); valid time: 0.09492s; loss: -30.8456 (±7.14492); valid loss: 56.2792\n",
      "[Epoch 250/256, Step 110300, ETA 9.827s] step time: 0.003006s (±0.009931s); valid time: 0.09967s; loss: -32.8898 (±7.40431); valid loss: 49.8064\n",
      "[Epoch 250/256, Step 110400, ETA 9.481s] step time: 0.003003s (±0.009646s); valid time: 0.0969s; loss: -31.452 (±7.26434); valid loss: 54.1458\n",
      "[Epoch 250/256, Step 110500, ETA 9.136s] step time: 0.002996s (±0.009751s); valid time: 0.09792s; loss: -31.6544 (±7.41015); valid loss: 59.5262\n",
      "[Epoch 250/256, Step 110500, ETA 9.136s] Learning rate decreased to 7.525434581650002e-07\n",
      "[Epoch 251/256, Step 110600, ETA 8.792s] step time: 0.002948s (±0.009626s); valid time: 0.0967s; loss: -32.9978 (±6.80527); valid loss: 49.4476\n",
      "[Epoch 251/256, Step 110700, ETA 8.447s] step time: 0.002971s (±0.009791s); valid time: 0.0983s; loss: -32.1864 (±6.89529); valid loss: 55.0947\n",
      "[Epoch 251/256, Step 110800, ETA 8.102s] step time: 0.003018s (±0.009794s); valid time: 0.09843s; loss: -32.3786 (±7.55994); valid loss: 56.2169\n",
      "[Epoch 251/256, Step 110900, ETA 7.758s] step time: 0.003153s (±0.01057s); valid time: 0.1062s; loss: -30.6951 (±7.07537); valid loss: 47.8588\n",
      "[Epoch 252/256, Step 111000, ETA 7.414s] step time: 0.003038s (±0.01002s); valid time: 0.1006s; loss: -30.7599 (±7.13616); valid loss: 48.7966\n",
      "[Epoch 252/256, Step 111100, ETA 7.069s] step time: 0.002995s (±0.009835s); valid time: 0.09879s; loss: -33.625 (±8.31689); valid loss: 54.5729\n",
      "[Epoch 252/256, Step 111200, ETA 6.724s] step time: 0.003107s (±0.01006s); valid time: 0.101s; loss: -31.4006 (±7.65442); valid loss: 59.103\n",
      "[Epoch 252/256, Step 111300, ETA 6.379s] step time: 0.003014s (±0.009903s); valid time: 0.0995s; loss: -31.8573 (±6.42675); valid loss: 51.8205\n",
      "[Epoch 253/256, Step 111400, ETA 6.036s] step time: 0.003025s (±0.009904s); valid time: 0.09949s; loss: -31.3783 (±7.63812); valid loss: 47.6949\n",
      "[Epoch 253/256, Step 111500, ETA 5.691s] step time: 0.003103s (±0.01023s); valid time: 0.1028s; loss: -31.1034 (±7.05026); valid loss: 50.4036\n",
      "[Epoch 253/256, Step 111600, ETA 5.346s] step time: 0.003085s (±0.00974s); valid time: 0.09785s; loss: -31.8877 (±7.90149); valid loss: 55.9779\n",
      "[Epoch 253/256, Step 111700, ETA 5.002s] step time: 0.003051s (±0.009949s); valid time: 0.09946s; loss: -32.181 (±6.78107); valid loss: 50.7471\n",
      "[Epoch 253/256, Step 111800, ETA 4.657s] step time: 0.003053s (±0.009944s); valid time: 0.09982s; loss: -32.3467 (±6.22984); valid loss: 58.7338\n",
      "[Epoch 254/256, Step 111900, ETA 4.313s] step time: 0.003108s (±0.01102s); valid time: 0.1106s; loss: -32.3741 (±7.27483); valid loss: 56.3336\n",
      "[Epoch 254/256, Step 112000, ETA 3.968s] step time: 0.003046s (±0.009764s); valid time: 0.09818s; loss: -30.7219 (±7.6237); valid loss: 48.0244\n",
      "[Epoch 254/256, Step 112100, ETA 3.624s] step time: 0.003027s (±0.009942s); valid time: 0.09989s; loss: -31.6647 (±6.96524); valid loss: 53.3431\n",
      "[Epoch 254/256, Step 112200, ETA 3.279s] step time: 0.003014s (±0.009915s); valid time: 0.09964s; loss: -31.4134 (±7.0834); valid loss: 49.7087\n",
      "[Epoch 255/256, Step 112300, ETA 2.935s] step time: 0.002957s (±0.009706s); valid time: 0.09746s; loss: -31.9608 (±8.03783); valid loss: 51.3781\n",
      "[Epoch 255/256, Step 112400, ETA 2.59s] step time: 0.00295s (±0.009567s); valid time: 0.09609s; loss: -31.4957 (±6.65951); valid loss: 59.514\n",
      "[Epoch 255/256, Step 112500, ETA 2.246s] step time: 0.002964s (±0.009719s); valid time: 0.09756s; loss: -31.8099 (±7.27328); valid loss: 51.7951\n",
      "[Epoch 255/256, Step 112600, ETA 1.901s] step time: 0.002986s (±0.009592s); valid time: 0.09633s; loss: -32.4478 (±7.05725); valid loss: 56.5236\n",
      "[Epoch 255/256, Step 112700, ETA 1.557s] step time: 0.00298s (±0.009714s); valid time: 0.09758s; loss: -31.1319 (±7.24826); valid loss: 61.084\n",
      "[Epoch 256/256, Step 112800, ETA 1.212s] step time: 0.002988s (±0.009646s); valid time: 0.09689s; loss: -32.8203 (±6.73374); valid loss: 54.5662\n",
      "[Epoch 256/256, Step 112900, ETA 0.8678s] step time: 0.003006s (±0.009754s); valid time: 0.09788s; loss: -31.4481 (±7.17766); valid loss: 56.862\n",
      "[Epoch 256/256, Step 113000, ETA 0.5234s] step time: 0.00305s (±0.01002s); valid time: 0.1006s; loss: -30.7258 (±7.19439); valid loss: 61.6911\n",
      "[Epoch 256/256, Step 113100, ETA 0.1791s] step time: 0.003048s (±0.01016s); valid time: 0.102s; loss: -32.4766 (±7.51927); valid loss: 58.1864\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpc256x_wt/variables.dat-20500\n"
     ]
    }
   ],
   "source": [
    "from donut import DonutTrainer, DonutPredictor\n",
    "\n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs)\n",
    "predictor = DonutPredictor(model)\n",
    "\n",
    "with tf.Session().as_default():\n",
    "    trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "    test_score = predictor.get_score(test_values, test_missing)\n",
    "    var_dict = get_variables_as_dict(model_vs)\n",
    "\n",
    "    # save variables to `save_dir`\n",
    "    save_dir = \"models/\"\n",
    "    saver = VariableSaver(var_dict, save_dir)\n",
    "    saver.save()"
   ]
  },
  {
   "source": [
    "# Restore the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/samsepiol/Desktop/ANM project/donut/temp/models/variables.dat\n"
     ]
    }
   ],
   "source": [
    "with tf.Session().as_default():\n",
    "    # Restore variables from `save_dir`.\n",
    "    saver = VariableSaver(get_variables_as_dict(model_vs), save_dir)\n",
    "    saver.restore()"
   ]
  }
 ]
}