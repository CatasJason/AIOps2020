{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from termcolor import colored\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from threading import Thread, Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESB_Analyzer():\n",
    "    def __init__(self, esb_data):\n",
    "        self.esb_data = esb_data\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        filename = 'birch_model_avgtime.sav'\n",
    "        self.avg_time_model = pickle.load(open(filename, 'rb'))\n",
    "        filename = 'birch_model_success.sav'\n",
    "        self.succee_rate_model = pickle.load(open(filename, 'rb'))\n",
    "        print('ESB models loaded')\n",
    "\n",
    "    def update_esb_data(self, esb_data):\n",
    "        self.esb_data = esb_data\n",
    "\n",
    "    def birch(self, values, ctype):  # values should be a list\n",
    "        X = np.reshape(values, (-1, 1))\n",
    "        brc = self.avg_time_model if ctype == \"time\" else self.succee_rate_model\n",
    "        return brc.predict(X)\n",
    "\n",
    "    def analyze_esb(self, esb_dict):\n",
    "        esb_tmp = self.esb_data.append(esb_dict, ignore_index=True)\n",
    "        values = esb_tmp['avg_time'].tolist()\n",
    "        # print(values)\n",
    "        birch_labels_time = self.birch(values,\"time\")\n",
    "        # birch_labels_rate = self.birch(self.esb_data['avg_time'])\n",
    "        for label in birch_labels_time:\n",
    "            if (label != 0):\n",
    "                print(\"Found esb_anomaly in avg_time\")\n",
    "                return True\n",
    "\n",
    "        values = esb_tmp['succee_rate'].tolist()\n",
    "        # print(values)\n",
    "        birch_labels_time = self.birch(values,\"rate\")\n",
    "        for label in birch_labels_time:\n",
    "            if (label != 0):\n",
    "                print(\"Found esb_anomaly in success rate\")\n",
    "                return True\n",
    "\n",
    "        self.update_esb_data(esb_tmp)\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class RCA():\n",
    "    def __init__(self, trace_data, host_data, alpha=0.95, ub=0.02, take_minute_averages_of_trace_data=True, division_milliseconds=60000):\n",
    "        self.trace_data = trace_data\n",
    "        self.host_data = host_data\n",
    "        self.alpha = alpha\n",
    "        self.ub = ub\n",
    "        self.anomaly_chart = None\n",
    "        self.take_minute_averages_of_trace_data = take_minute_averages_of_trace_data\n",
    "        self.division_milliseconds = division_milliseconds\n",
    "\n",
    "    def run(self):\n",
    "        self.trace_processing()\n",
    "        \n",
    "        print('Running RCA on %d trace data rows and %d host data rows' %\n",
    "              (len(self.trace_data), len(self.host_data)))\n",
    "        overall_start_time = time.time()\n",
    "\n",
    "        print('Computing for anomaly score chart...')\n",
    "        self.hesd_trace_detection(alpha=self.alpha, ub=self.ub)\n",
    "        print('Score chart computed!')\n",
    "\n",
    "        self.local_initiate()\n",
    "        output = self.find_anomalous_rows()\n",
    "\n",
    "        print('The output to send to the server is: ' +\n",
    "              colored(str(output), 'magenta'))\n",
    "\n",
    "        print('RCA finished in ' + colored('%f', 'cyan') %\n",
    "              (time.time() - overall_start_time) + ' seconds.')\n",
    "        return output\n",
    "\n",
    "    def esd_test_statistics(self, x, hybrid=True):\n",
    "        \"\"\"\n",
    "        Compute the location and dispersion sample statistics used to carry out the ESD test.\n",
    "        \"\"\"\n",
    "        if hybrid:\n",
    "            location = pd.Series(x).median(skipna=True) # Median\n",
    "            dispersion = np.median(np.abs(x - np.median(x))) # Median Absolute Deviation\n",
    "        else:  \n",
    "            location = pd.Series(x).mean(skipna=True) # Mean\n",
    "            dispersion = pd.Series(x).std(skipna=True) # Standard Deviation\n",
    "            \n",
    "        return location, dispersion\n",
    "\n",
    "    def esd_test(self, x, alpha=0.95, ub=0.499, hybrid=True):\n",
    "        \"\"\"\n",
    "        Carries out the Extreme Studentized Deviate(ESD) test which can be used to detect one or more outliers present in the timeseries\n",
    "        \n",
    "        x      : List, array, or series containing the time series\n",
    "        freq   : Int that gives the number of periods per cycle (7 for week, 12 for monthly, etc)\n",
    "        alpha  : Confidence level in detecting outliers\n",
    "        ub     : Upper bound on the fraction of datapoints which can be labeled as outliers (<=0.499)\n",
    "        hybrid : Whether to use the robust statistics (median, median absolute error) or the non-robust versions (mean, standard deviation) to test for anomalies\n",
    "        \"\"\"\n",
    "        nobs = len(x)\n",
    "        if ub > 0.4999:\n",
    "            ub = 0.499\n",
    "        k = max(int(np.floor(ub * nobs)), 1) # Maximum number of anomalies. At least 1 anomaly must be tested.\n",
    "        #   res_tmp = ts_S_Md_decomposition(x)[\"residual\"] # Residuals from time series decomposition\n",
    "            \n",
    "        # Carry out the esd test k times  \n",
    "        res = np.ma.array(x, mask=False) # The \"ma\" structure allows masking of values to exclude the elements from any calculation\n",
    "        anomalies = [] # returns the indices of the found anomalies\n",
    "        med = np.median(x)\n",
    "        for i in range(1, k+1):\n",
    "            location, dispersion = self.esd_test_statistics(res, hybrid) # Sample statistics\n",
    "            tmp = np.abs(res - location) / dispersion\n",
    "            idx = np.argmax(tmp) # Index of the test statistic\n",
    "            test_statistic = tmp[idx] \n",
    "            n = nobs - res.mask.sum() # sums  nonmasked values\n",
    "            critical_value = (n - i) * t.ppf(alpha, n - i - 1) / np.sqrt((n - i - 1 + np.power(t.ppf(alpha, n - i - 1), 2)) * (n - i - 1)) \n",
    "            if test_statistic > critical_value:\n",
    "                anomalies.append((x[idx]-med)/med)\n",
    "                # anomalies.append(test_statistic)\n",
    "            res.mask[idx] = True\n",
    "        if len(anomalies) == 0:\n",
    "            return 0\n",
    "        return np.mean(np.abs(anomalies))\n",
    "    \n",
    "    def hesd_trace_detection(self, alpha=0.95, ub=0.02):\n",
    "        grouped_df = self.trace_data.groupby(['cmdb_id', 'serviceName'])[['startTime','actual_time']]\n",
    "\n",
    "        self.anomaly_chart = pd.DataFrame()\n",
    "        for (a, b), value in grouped_df:\n",
    "            value['time_group'] = value.startTime//self.division_milliseconds\n",
    "            value = value.groupby(['time_group'])['actual_time'].mean().reset_index()\n",
    "            result = self.esd_test(value['actual_time'].to_numpy(), alpha=alpha, ub=ub, hybrid=True)\n",
    "            self.anomaly_chart.loc[b,a] = result\n",
    "\n",
    "        self.anomaly_chart = self.anomaly_chart.sort_index()\n",
    "        print(self.anomaly_chart)\n",
    "        # print(self.anomaly_chart.to_dict())\n",
    "        return self.anomaly_chart\n",
    "    \n",
    "    def local_initiate(self):\n",
    "        self.dockers = ['docker_001', 'docker_002', 'docker_003', 'docker_004',\n",
    "                'docker_005', 'docker_006', 'docker_007', 'docker_008']\n",
    "        self.docker_hosts = ['os_017', 'os_018', 'os_019', 'os_020']\n",
    "\n",
    "        self.docker_kpi_names = ['container_cpu_used', None]\n",
    "        self.os_kpi_names = ['Sent_queue', 'Received_queue']\n",
    "        self.db_kpi_names = ['Proc_User_Used_Pct','Proc_Used_Pct','Sess_Connect','On_Off_State', 'tnsping_result_time']\n",
    "\n",
    "        self.docker_lookup_table = {}\n",
    "        for i in range(len(self.dockers)):\n",
    "            self.docker_lookup_table[self.dockers[i]] = self.docker_hosts[i % 4]\n",
    "\n",
    "\n",
    "    def find_anomalous_rows(self, min_threshold = 5):\n",
    "        table = self.anomaly_chart.copy()\n",
    "        threshold = max( 0.5 * table.stack().max(), min_threshold)\n",
    "        dodgy_rows = []\n",
    "        just_rows = []\n",
    "        row_col_dict = {}\n",
    "        for column in table:\n",
    "            v = 0\n",
    "            r = ''\n",
    "            for index, row in table.iterrows():\n",
    "                if (row[column] > threshold):\n",
    "                    if index == column:\n",
    "                        dodgy_rows.append([index, column, row[column]])\n",
    "                        just_rows.append(index)\n",
    "                        row_col_dict[index] = True\n",
    "                        break\n",
    "                    elif (row[column] > v):\n",
    "                        v = row[column]\n",
    "                        r = index\n",
    "            if r != '':\n",
    "                dodgy_rows.append([r, column, v])\n",
    "                just_rows.append(r)\n",
    "                if r not in row_col_dict.keys():\n",
    "                    row_col_dict[r] = False\n",
    "        output = self.localize(dodgy_rows, list(set(just_rows)), row_col_dict)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def find_anomalous_kpi(self, cmdb_id, row_col_same = False):\n",
    "        kpi_names = []\n",
    "        if 'os' in cmdb_id:\n",
    "            kpi_names = self.os_kpi_names\n",
    "        elif 'docker' in cmdb_id:\n",
    "            kpi_names = [self.docker_kpi_names[0]] if row_col_same else [self.docker_kpi_names[1]]\n",
    "        else:\n",
    "            kpi_names = self.db_kpi_names\n",
    "            host_data_subset = self.host_data.loc[(self.host_data.cmdb_id == cmdb_id) & (self.host_data.name.isin(kpi_names))]\n",
    "            results_dict = {}\n",
    "            for kpi, values in host_data_subset.groupby('name')['value']:\n",
    "                values = list(values)\n",
    "                score =  self.esd_test(np.array(values), 0.95, 0.1, True)\n",
    "                results_dict[kpi] = score\n",
    "            db_connection_limit = [results_dict['Proc_User_Used_Pct'],results_dict['Proc_Used_Pct'],results_dict['Sess_Connect']]\n",
    "            db_connection_limit_score = np.mean(db_connection_limit)\n",
    "            db_close = [results_dict['On_Off_State'],results_dict['tnsping_result_time']]\n",
    "            db_close_score = np.mean(db_close)\n",
    "            if db_connection_limit_score > db_close_score:\n",
    "                kpi_names = kpi_names[:3]\n",
    "            else:\n",
    "                kpi_names = kpi_names[3:]             \n",
    "\n",
    "        return kpi_names\n",
    "\n",
    "\n",
    "    def localize(self, dodgy_rows, just_rows, row_col_dict):\n",
    "        n = len(just_rows)\n",
    "        print('%d anomalies found' % n)\n",
    "        if n < 1:\n",
    "            return None\n",
    "        if n == 1:\n",
    "            KPIs = self.find_anomalous_kpi(just_rows[0], row_col_dict[list(row_col_dict.keys())[0]])\n",
    "            to_be_sent = []\n",
    "            for KPI in KPIs:\n",
    "                to_be_sent.append([just_rows[0], KPI])\n",
    "            return to_be_sent\n",
    "        if n == 2:\n",
    "            r0 = just_rows[0]\n",
    "            r1 = just_rows[1]\n",
    "            if ('os' in r0) and ('os' in r1):\n",
    "                KPIS = self.find_anomalous_kpi('os_001')\n",
    "                return [['os_001', KPIS[0]], ['os_001', KPIS[1]]]\n",
    "\n",
    "            if ('docker' in r0) and ('docker' in r1):\n",
    "                if self.docker_lookup_table[r0] == self.docker_lookup_table[r1]:\n",
    "                    KPIS = self.find_anomalous_kpi(self.docker_lookup_table[r0])\n",
    "                    to_be_sent = []\n",
    "                    for kpi in KPIS:\n",
    "                        to_be_sent.append([self.docker_lookup_table[r0], kpi])\n",
    "                    return to_be_sent\n",
    "\n",
    "            KPI0s = self.find_anomalous_kpi(r0, row_col_dict[r0])\n",
    "            KPI1s = self.find_anomalous_kpi(r1, row_col_dict[r1])\n",
    "            to_be_sent = []\n",
    "            for kpi in KPI0s:\n",
    "                to_be_sent.append([r0, kpi])\n",
    "            for kpi in KPI1s:\n",
    "                to_be_sent.append([r1, kpi])\n",
    "            return to_be_sent\n",
    "        if n > 2:\n",
    "            dodgy_rows.sort(key = lambda x: x[2], reverse = True)\n",
    "            just_rows = [x[0] for x in dodgy_rows]\n",
    "            just_rows = list(np.unique(just_rows))\n",
    "            row_col_dict = { just_rows[0]: row_col_dict[just_rows[0]], just_rows[1]: row_col_dict[just_rows[1]] } \n",
    "            return self.localize(dodgy_rows[:2], just_rows[:2], row_col_dict)\n",
    "\n",
    "\n",
    "    def update_trace_data(self, trace_data):\n",
    "        self.trace_data = trace_data\n",
    "\n",
    "    def update_host_data(self, host_data):\n",
    "        self.host_data = host_data\n",
    "\n",
    "    def trace_processing(self):\n",
    "        print(\"Started trace processing\")\n",
    "        p_time = time.time()\n",
    "        self.trace_data = self.trace_data[self.trace_data['callType'] != 'FlyRemote'].copy()\n",
    "        df1 = self.trace_data[self.trace_data['callType']=='RemoteProcess']\n",
    "        df1 = df1[['pid','cmdb_id']]\n",
    "        df1 = df1.set_index('pid')\n",
    "\n",
    "        csf_cmdb = df1.to_dict()\n",
    "        csf_cmdb = {str(key):str(values) for key, values in csf_cmdb['cmdb_id'].items()}\n",
    "\n",
    "        # for index, row in self.trace_data.iterrows():\n",
    "        #     if row['id'] in csf_cmdb:\n",
    "        #         self.trace_data.at[index, 'serviceName'] = csf_cmdb[row['id']]\n",
    "\n",
    "        elapse_time = {}\n",
    "        children = {}\n",
    "\n",
    "        def do_thing(row):\n",
    "            ## if change 297 and 298, gets different result??? loook into this plz\n",
    "            if row['id'] in csf_cmdb:\n",
    "                row['serviceName'] = csf_cmdb[row['id']]\n",
    "            if row['pid'] != 'None':\n",
    "                children[row['pid']] = children.get(row['pid'], [])\n",
    "                children[row['pid']].append(row['id'])\n",
    "            elapse_time[row['id']] = float(row['elapsedTime'])\n",
    "            return row\n",
    "        self.trace_data = self.trace_data.apply(do_thing, axis=1)\n",
    "\n",
    "\n",
    "        # for index, row in self.trace_data.iterrows():\n",
    "        #     if row['pid'] != 'None':\n",
    "        #         if row['pid'] in children.keys():\n",
    "        #             children[row['pid']].append(row['id'])\n",
    "        #         else:\n",
    "        #             children[row['pid']] = [row['id']]\n",
    "        #     elapse_time[row['id']] = float(row['elapsedTime'])\n",
    "        \n",
    "        # self.trace_data.apply(parse, axis = 1)\n",
    "\n",
    "        self.trace_data['actual_time'] = 0.0\n",
    "        # for index, row in self.trace_data.iterrows():\n",
    "        #     total_child = 0.0\n",
    "        #     if row['id'] not in children.keys():\n",
    "        #         self.trace_data.at[index, 'actual_time'] = row['elapsedTime']\n",
    "        #         continue\n",
    "        #     for child in children[row['id']]:\n",
    "        #         total_child += elapse_time[child]\n",
    "        #     self.trace_data.at[index, 'actual_time'] = row['elapsedTime'] - total_child\n",
    "\n",
    "        def get_actual_time(row):\n",
    "            total_child = 0.0\n",
    "            if row['id'] in children:\n",
    "                for child in children[row['id']]:\n",
    "                    total_child += elapse_time[child]\n",
    "            row['actual_time'] = row['elapsedTime'] - total_child\n",
    "            return row\n",
    "        \n",
    "        self.trace_data = self.trace_data.apply(get_actual_time, axis = 1)\n",
    "        \n",
    "        self.trace_data = self.trace_data[~(self.trace_data['serviceName'].str.contains('csf', na=True))]\n",
    "\n",
    "        print(\"Trace processed in \", time.time()-p_time, 'seconds')\n",
    "        print(self.trace_data)\n",
    "\n",
    "\n",
    "# Three topics are available: platform-index, business-index, trace.\n",
    "# Subscribe at least one of them.\n",
    "AVAILABLE_TOPICS = set(['platform-index', 'business-index', 'trace'])\n",
    "# CONSUMER = KafkaConsumer('platform-index', 'business-index', 'trace',\n",
    "#                          bootstrap_servers=['172.21.0.8', ],\n",
    "#                          auto_offset_reset='latest',\n",
    "#                          enable_auto_commit=False,\n",
    "#                          security_protocol='PLAINTEXT')\n",
    "\n",
    "\n",
    "class Trace():  # pylint: disable=invalid-name,too-many-instance-attributes,too-few-public-methods\n",
    "    '''Structure for traces'''\n",
    "\n",
    "    __slots__ = ['call_type', 'start_time', 'elapsed_time', 'success',\n",
    "                 'trace_id', 'id', 'pid', 'cmdb_id', 'service_name', 'ds_name']\n",
    "\n",
    "    def __new__(self, data):\n",
    "        self.trace = data\n",
    "        if self.trace['callType'] == 'JDBC' or self.trace['callType']=='LOCAL':\n",
    "            try:\n",
    "                self.trace['serviceName'] = data['dsName']\n",
    "            except:\n",
    "                print(data)\n",
    "                print('JDBC doesnt have dsName')\n",
    "        \n",
    "        elif self.trace['callType']=='RemoteProcess' or self.trace['callType']=='OSB':\n",
    "            self.trace['serviceName'] = data['cmdb_id']\n",
    "\n",
    "        if 'dsName' in self.trace:\n",
    "            self.trace.pop('dsName')\n",
    "\n",
    "        return self.trace\n",
    "\n",
    "\n",
    "def detection(timestamp):\n",
    "    global host_df, trace_df\n",
    "    print('Starting Anomaly Detection')\n",
    "    startTime = timestamp - 1200000  # one minute before anomaly\n",
    "\n",
    "    # print(len(trace_df), trace_df.head())\n",
    "    # print(len(host_df), host_df.head())\n",
    "    trace_df_temp = trace_df[(trace_df['startTime'] >= startTime) &\n",
    "                             (trace_df['startTime'] <= timestamp)]\n",
    "    host_df_temp = host_df[(host_df['timestamp'] >= startTime) &\n",
    "                           (host_df['timestamp'] <= timestamp)]\n",
    "    print(len(trace_df_temp), trace_df_temp.head())\n",
    "    print(len(host_df_temp), host_df_temp.head())\n",
    "\n",
    "    rca_temp = RCA(trace_data=trace_df_temp, host_data=host_df_temp)\n",
    "    results_to_send_off = rca_temp.run()\n",
    "\n",
    "    print('Anomaly Detection Done.')\n",
    "    if results_to_send_off is None:\n",
    "        # print('Nothing detected')\n",
    "        return False, rca_temp\n",
    "    # for a in anom_hosts:\n",
    "    #     item = a.split(':')[0]\n",
    "    #     if (item not in anoms):\n",
    "    #         anoms.append(item)\n",
    "    # print(results_to_send_off)\n",
    "    # submit(results_to_send_off)\n",
    "    return True, rca_temp\n",
    "\n",
    "\n",
    "def rcaprocess(esb_item, trace, host, timestamp, lock):\n",
    "    global host_df, trace_df, esb_anal, a_time\n",
    "    esb_anomaly = False\n",
    "\n",
    "    # print(trace)\n",
    "    trace_df = trace_df[(trace_df.startTime >= (timestamp-1260000))]\n",
    "    host_df = host_df[(host_df.timestamp >= (timestamp-1260000))]\n",
    "    \n",
    "    t = time.time()\n",
    "    t_df = pd.DataFrame(trace)\n",
    "    h_df = pd.DataFrame(host)\n",
    "\n",
    "    trace_df = pd.concat([trace_df, t_df], axis=0, ignore_index=True)\n",
    "    host_df = pd.concat([host_df, h_df], axis=0, ignore_index=True)\n",
    "\n",
    "    print('Time to add new data: ', (time.time()-t))\n",
    "\n",
    "    print(esb_anal.esb_data.tail(1))\n",
    "    print(host_df.tail(1))\n",
    "    print(trace_df.tail(1))\n",
    "\n",
    "    with lock:\n",
    "        esb_anomaly = esb_anal.analyze_esb(esb_item)\n",
    "        if (time.time() - a_time) >= 600 and esb_anomaly:\n",
    "            tmp_time = time.time()\n",
    "            print(\"oops\")\n",
    "            # detection(timestamp)\n",
    "            result = detection(timestamp)\n",
    "            print('Anomaly at: ', timestamp)\n",
    "            if result:\n",
    "                a_time = tmp_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Unnamed: 0           itemid                  name     bomc_id  \\\n",
      "0           72841  999999996432420            Send_total  ZJ-001-074   \n",
      "1          464106  999999998650680          CPU_free_pct  ZJ-002-056   \n",
      "2          464107  999999998650980          CPU_free_pct  ZJ-002-056   \n",
      "3          464108  999999998651100         MEM_real_util  ZJ-002-053   \n",
      "4          464105  999999998651280          CPU_free_pct  ZJ-002-056   \n",
      "...           ...              ...                   ...         ...   \n",
      "41583       93814  999999996432420            Send_total  ZJ-001-074   \n",
      "41584      476252  999999998650980          CPU_free_pct  ZJ-002-056   \n",
      "41585      357294  999999996508507             maxmemory  ZJ-005-020   \n",
      "41586       93823  999999996431520  Memory_available_pct  ZJ-001-017   \n",
      "41587      476253  999999998651100         MEM_real_util  ZJ-002-053   \n",
      "\n",
      "           timestamp         value    cmdb_id  \n",
      "0      1590341220000  5.520000e-04     os_008  \n",
      "1      1590341220000  9.872038e+01     db_001  \n",
      "2      1590341220000  9.892670e+01     db_003  \n",
      "3      1590341220000  8.183000e+01     db_007  \n",
      "4      1590341220000  9.772938e+01     db_008  \n",
      "...              ...           ...        ...  \n",
      "41583  1590342720000  5.610000e-04     os_008  \n",
      "41584  1590342720000  9.884418e+01     db_003  \n",
      "41585  1590342720000  2.000000e+09  redis_007  \n",
      "41586  1590342720000  5.965000e+01     os_011  \n",
      "41587  1590342720000  8.184000e+01     db_007  \n",
      "\n",
      "[41588 rows x 7 columns]\n",
      "Starting Anomaly Detection\n",
      "617471        Unnamed: 0 callType      startTime  elapsedTime  success  \\\n",
      "67527    10057016      OSB  1590341340031        667.0     True   \n",
      "67528    10057017      OSB  1590341340031        358.0     True   \n",
      "67529    10057018      OSB  1590341340031        356.0     True   \n",
      "67530    10057020      OSB  1590341340033       1318.0     True   \n",
      "67531    10057019      OSB  1590341340033        355.0     True   \n",
      "\n",
      "                    traceId                    id   pid cmdb_id serviceName  \n",
      "67527  2a171b6cc84573f7b8fe  2a17168f235573f7b8fe  None  os_021      os_021  \n",
      "67528  2a171c26676105f7b8fe  2a1714e6d77105f7b8fe  None  os_021      os_021  \n",
      "67529  2a171875480493f7b8fe  2a171e7bba1493f7b8fe  None  os_021      os_021  \n",
      "67530  2a1712d3e4205518b8fe  2a171a252a305518b8fe  None  os_022      os_022  \n",
      "67531  2a171f1fa4020518b8fe  2a1715784f120518b8fe  None  os_021      os_021  \n",
      "33100       Unnamed: 0           itemid                       name     bomc_id  \\\n",
      "3612      465350  999999998650680               CPU_free_pct  ZJ-002-056   \n",
      "3613      465351  999999998650980               CPU_free_pct  ZJ-002-056   \n",
      "3614      465349  999999998651280               CPU_free_pct  ZJ-002-056   \n",
      "3615      465352  999999998651100              MEM_real_util  ZJ-002-053   \n",
      "3616       74533  999999996431640  System_block_queue_length  ZJ-001-077   \n",
      "\n",
      "          timestamp      value cmdb_id  \n",
      "3612  1590341340000  98.521472  db_001  \n",
      "3613  1590341340000  99.014982  db_003  \n",
      "3614  1590341340000  97.327936  db_008  \n",
      "3615  1590341340000  81.830000  db_007  \n",
      "3616  1590341340000   0.000000  os_004  \n",
      "Started trace processing\n",
      "Trace processed in  62.47210764884949 seconds\n",
      "        Unnamed: 0 callType      startTime  elapsedTime  success  \\\n",
      "67527     10057016      OSB  1590341340031        667.0     True   \n",
      "67528     10057017      OSB  1590341340031        358.0     True   \n",
      "67529     10057018      OSB  1590341340031        356.0     True   \n",
      "67530     10057020      OSB  1590341340033       1318.0     True   \n",
      "67531     10057019      OSB  1590341340033        355.0     True   \n",
      "...            ...      ...            ...          ...      ...   \n",
      "684992     7613131    LOCAL  1590342538363       1998.0     True   \n",
      "684993     3382989     JDBC  1590342538364       1997.0     True   \n",
      "684995     7613132    LOCAL  1590342538662       1996.0     True   \n",
      "684996     3382990     JDBC  1590342538664       1993.0     True   \n",
      "684997     3382991     JDBC  1590342538671       2005.0     True   \n",
      "\n",
      "                     traceId                    id                   pid  \\\n",
      "67527   2a171b6cc84573f7b8fe  2a17168f235573f7b8fe                  None   \n",
      "67528   2a171c26676105f7b8fe  2a1714e6d77105f7b8fe                  None   \n",
      "67529   2a171875480493f7b8fe  2a171e7bba1493f7b8fe                  None   \n",
      "67530   2a1712d3e4205518b8fe  2a171a252a305518b8fe                  None   \n",
      "67531   2a171f1fa4020518b8fe  2a1715784f120518b8fe                  None   \n",
      "...                      ...                   ...                   ...   \n",
      "684992  3a171c7a3f8796065a00  3a171f74656307b74d10  3a17142ee2773653da00   \n",
      "684993  3a171c7a3f8796065a00  3a171dd7e69307c74d10  3a171f74656307b74d10   \n",
      "684995  3a171431726993190f00  3a1716a4d934076a5d10  3a17104a305056d68f00   \n",
      "684996  3a171431726993190f00  3a171bc49184078a5d10  3a1716a4d934076a5d10   \n",
      "684997  3a171366350292390f00  3a171d16f11507fa5d10  3a1719aa1192072ddc10   \n",
      "\n",
      "           cmdb_id serviceName  actual_time  \n",
      "67527       os_021      os_021         18.0  \n",
      "67528       os_021      os_021         19.0  \n",
      "67529       os_021      os_021         24.0  \n",
      "67530       os_022      os_022         42.0  \n",
      "67531       os_021      os_021         22.0  \n",
      "...            ...         ...          ...  \n",
      "684992  docker_004      db_009          1.0  \n",
      "684993  docker_004      db_009       1997.0  \n",
      "684995  docker_004      db_009          3.0  \n",
      "684996  docker_004      db_009       1993.0  \n",
      "684997  docker_004      db_009       2005.0  \n",
      "\n",
      "[606804 rows x 11 columns]\n",
      "Running RCA on 606804 trace data rows and 33100 host data rows\n",
      "Computing for anomaly score chart...\n",
      "            docker_001  docker_002  docker_003   docker_004  docker_005  \\\n",
      "db_003             NaN         NaN         NaN          NaN    0.541179   \n",
      "db_007        0.766416    0.767688    0.630604   202.305539         NaN   \n",
      "db_009        0.900208    0.856043    0.644354   335.058678         NaN   \n",
      "docker_001   12.265547         NaN         NaN          NaN         NaN   \n",
      "docker_002         NaN    0.855549         NaN          NaN         NaN   \n",
      "docker_003         NaN         NaN    0.604130          NaN         NaN   \n",
      "docker_004         NaN         NaN         NaN  1193.199250         NaN   \n",
      "docker_005         NaN         NaN    0.802577   266.652861    0.267735   \n",
      "docker_006         NaN         NaN    0.674232   238.874064         NaN   \n",
      "docker_007    0.888688    0.825206         NaN          NaN         NaN   \n",
      "docker_008  159.536231  217.396751         NaN          NaN         NaN   \n",
      "os_021             NaN         NaN         NaN          NaN         NaN   \n",
      "os_022             NaN         NaN         NaN          NaN         NaN   \n",
      "\n",
      "            docker_006  docker_007  docker_008      os_021    os_022  \n",
      "db_003        0.659585    0.052353  274.069062         NaN       NaN  \n",
      "db_007             NaN         NaN         NaN         NaN       NaN  \n",
      "db_009             NaN         NaN         NaN         NaN       NaN  \n",
      "docker_001         NaN         NaN         NaN         NaN  3.615975  \n",
      "docker_002         NaN         NaN         NaN         NaN  7.180581  \n",
      "docker_003         NaN         NaN         NaN    1.742957       NaN  \n",
      "docker_004         NaN         NaN         NaN  198.887442       NaN  \n",
      "docker_005         NaN         NaN         NaN         NaN       NaN  \n",
      "docker_006    0.277947         NaN         NaN         NaN       NaN  \n",
      "docker_007         NaN    0.114516         NaN         NaN       NaN  \n",
      "docker_008         NaN         NaN   70.715548         NaN       NaN  \n",
      "os_021             NaN         NaN         NaN    0.385985       NaN  \n",
      "os_022             NaN         NaN         NaN         NaN  2.414383  \n",
      "Score chart computed!\n",
      "1 anomalies found\n",
      "The output to send to the server is: \u001b[35m[['docker_004', 'container_cpu_used']]\u001b[0m\n",
      "RCA finished in \u001b[36m0.227472\u001b[0m seconds.\n",
      "Anomaly Detection Done.\n"
     ]
    }
   ],
   "source": [
    "global host_df, trace_df\n",
    "\n",
    "path = 'training_data/'\n",
    "trace_df = pd.read_csv(path + 'trace_data_os20_new.csv')\n",
    "# trace_df = trace_df.drop(['actual_time','path'], axis=1)\n",
    "# trace_df = trace_df.drop(['path'], axis=1)\n",
    "trace_df = trace_df.sort_values(by=['startTime'], ignore_index=True)\n",
    "# trace = trace[trace.startTime < trace.startTime[0]+1260000]\n",
    "\n",
    "host_df = pd.read_csv(path + 'kpi_data_os20_new.csv')\n",
    "host_df = host_df.sort_values(by=['timestamp'], ignore_index=True)\n",
    "\n",
    "# print(trace_df)\n",
    "print(host_df)\n",
    "timestamp = int(host_df['timestamp'].iloc[-1]-180000)\n",
    "# print(timestamp)\n",
    "# trace_df = trace_df[(trace_df.startTime >= (timestamp-1260000))]\n",
    "# print(host_df)\n",
    "boo, rca = detection(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            docker_001  docker_002  docker_003  docker_004  docker_005  \\\n",
       "db_003             NaN         NaN         NaN         NaN    3.250200   \n",
       "db_007        3.816850    3.100102    4.058389    2.727318         NaN   \n",
       "db_009        2.206155    3.327810    4.687437    3.465393         NaN   \n",
       "docker_001    2.142233         NaN         NaN         NaN         NaN   \n",
       "docker_002         NaN    3.609478         NaN         NaN         NaN   \n",
       "docker_003         NaN         NaN    4.050388         NaN         NaN   \n",
       "docker_004         NaN         NaN         NaN    2.799524         NaN   \n",
       "docker_005    3.187608    3.688489         NaN         NaN    3.212034   \n",
       "docker_006    2.557572    2.644478         NaN         NaN         NaN   \n",
       "docker_007         NaN         NaN    3.975099    3.877159         NaN   \n",
       "docker_008         NaN         NaN    3.749392    3.105576         NaN   \n",
       "os_021             NaN         NaN         NaN         NaN         NaN   \n",
       "os_022             NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "            docker_006  docker_007  docker_008    os_021     os_022  \n",
       "db_003        3.046804    4.895393    3.554057       NaN        NaN  \n",
       "db_007             NaN         NaN         NaN       NaN        NaN  \n",
       "db_009             NaN         NaN         NaN       NaN        NaN  \n",
       "docker_001         NaN         NaN         NaN       NaN   4.567240  \n",
       "docker_002         NaN         NaN         NaN       NaN  15.164564  \n",
       "docker_003         NaN         NaN         NaN  3.997487        NaN  \n",
       "docker_004         NaN         NaN         NaN  7.526117        NaN  \n",
       "docker_005         NaN         NaN         NaN       NaN        NaN  \n",
       "docker_006    3.730045         NaN         NaN       NaN        NaN  \n",
       "docker_007         NaN    2.807617         NaN       NaN        NaN  \n",
       "docker_008         NaN         NaN    3.875794       NaN        NaN  \n",
       "os_021             NaN         NaN         NaN  6.220622        NaN  \n",
       "os_022             NaN         NaN         NaN       NaN  61.823462  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>docker_001</th>\n      <th>docker_002</th>\n      <th>docker_003</th>\n      <th>docker_004</th>\n      <th>docker_005</th>\n      <th>docker_006</th>\n      <th>docker_007</th>\n      <th>docker_008</th>\n      <th>os_021</th>\n      <th>os_022</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>db_003</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.250200</td>\n      <td>3.046804</td>\n      <td>4.895393</td>\n      <td>3.554057</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>db_007</th>\n      <td>3.816850</td>\n      <td>3.100102</td>\n      <td>4.058389</td>\n      <td>2.727318</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>db_009</th>\n      <td>2.206155</td>\n      <td>3.327810</td>\n      <td>4.687437</td>\n      <td>3.465393</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_001</th>\n      <td>2.142233</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.567240</td>\n    </tr>\n    <tr>\n      <th>docker_002</th>\n      <td>NaN</td>\n      <td>3.609478</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>15.164564</td>\n    </tr>\n    <tr>\n      <th>docker_003</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.050388</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.997487</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_004</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.799524</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.526117</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_005</th>\n      <td>3.187608</td>\n      <td>3.688489</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.212034</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_006</th>\n      <td>2.557572</td>\n      <td>2.644478</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.730045</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_007</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.975099</td>\n      <td>3.877159</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.807617</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_008</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.749392</td>\n      <td>3.105576</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.875794</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>os_021</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.220622</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>os_022</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>61.823462</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 669
    }
   ],
   "source": [
    "table = pd.DataFrame({'docker_001': {'db_003': np.nan, 'db_007': 3.816849753370544, 'db_009': 2.206154521337853, 'docker_001': 2.142232997274582, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': 3.187607705698747, 'docker_006': 2.5575722190051575, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_002': {'db_003': np.nan, 'db_007': 3.100101502564873, 'db_009': 3.3278100110710787, 'docker_001': np.nan, 'docker_002': 3.6094782207398035, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': 3.6884894164980517, 'docker_006': 2.6444777417663405, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_003': {'db_003': np.nan, 'db_007': 4.05838909056681, 'db_009': 4.687436749404859, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': 4.050388459687843, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': 3.975099415211639, 'docker_008': 3.749392209974092, 'os_021': np.nan, 'os_022': np.nan}, 'docker_004': {'db_003': np.nan, 'db_007': 2.7273183968102557, 'db_009': 3.465393061173016, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': 2.7995237664558155, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': 3.8771590820887587, 'docker_008': 3.105575829268132, 'os_021': np.nan, 'os_022': np.nan}, 'docker_005': {'db_003': 3.2501995024270998, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': 3.2120337362417577, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_006': {'db_003': 3.0468039042707815, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': 3.7300453386173498, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_007': {'db_003': 4.895393129420258, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': 2.807617486231632, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_008': {'db_003': 3.554056910446389, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': 3.875794475602469, 'os_021': np.nan, 'os_022': np.nan}, 'os_021': {'db_003': np.nan, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': 3.997486831290767, 'docker_004': 7.526116838074162, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': 6.220622213001989, 'os_022': np.nan}, 'os_022': {'db_003': np.nan, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': 4.56724006037287, 'docker_002': 15.164564064997037, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': 61.82346209522774}}\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['os_022']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['os_022', 'os_022', 61.82346209522774]]"
      ]
     },
     "metadata": {},
     "execution_count": 670
    }
   ],
   "source": [
    "threshold = max( 0.5 * table.stack().max(), 5)\n",
    "dodgy_rows = []\n",
    "just_rows = []\n",
    "row_col_dict = {}\n",
    "for column in table:\n",
    "    v = 0\n",
    "    r = ''\n",
    "    for index, row in table.iterrows():\n",
    "        if (row[column] > threshold):\n",
    "            if index == column:\n",
    "                dodgy_rows.append([index, column, row[column]])\n",
    "                just_rows.append(index)\n",
    "                row_col_dict[index] = True\n",
    "                break\n",
    "            elif (row[column] > v):\n",
    "                v = row[column]\n",
    "                r = index\n",
    "    if r != '':\n",
    "        dodgy_rows.append([r, column, v])\n",
    "        just_rows.append(r)\n",
    "        if r not in row_col_dict.keys():\n",
    "            row_col_dict[r] = False\n",
    "# output = self.localize(dodgy_rows, list(set(just_rows)), row_col_dict)\n",
    "print(list(set(just_rows)))\n",
    "dodgy_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nyhat  [-1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1 -1 -1 -1 -1]\n\nsec  [6.220622213001989, 7.526116838074162, 15.164564064997037]\n\nthresh  1.305494625072173\n\nrow col dict {'db_007': False, 'db_009': False, 'docker_001': True, 'docker_005': True, 'docker_006': True, 'docker_002': True, 'docker_003': True, 'docker_007': True, 'docker_008': True, 'docker_004': True, 'db_003': False, 'os_021': True, 'os_022': True}\n\nrow dict {'db_007': 4, 'db_009': 4, 'docker_001': 3, 'docker_005': 2, 'docker_006': 2, 'docker_002': 3, 'docker_003': 3, 'docker_007': 2, 'docker_008': 2, 'docker_004': 3, 'db_003': 4, 'os_021': 2, 'os_022': 2}\n\n\nyhat  [-1 -1  1  1  1  1  1 -1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['os_022', 'docker_002', 'docker_008']"
      ]
     },
     "metadata": {},
     "execution_count": 684
    }
   ],
   "source": [
    "import operator\n",
    "values = sorted(table.stack().tolist())\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# identify outliers in the training dataset\n",
    "X = np.reshape(values, (-1, 1))\n",
    "iso = IsolationForest()\n",
    "yhat = iso.fit_predict(X)\n",
    "print('\\nyhat ', yhat)\n",
    "\n",
    "import statistics\n",
    "\n",
    "thresh_idx = 0\n",
    "seen_normal = False\n",
    "for i in range(len(yhat)):\n",
    "    if (yhat[i] == 1):\n",
    "        seen_normal = True\n",
    "    elif (yhat[i] == -1) and seen_normal:\n",
    "        thresh_idx = i\n",
    "        break\n",
    "\n",
    "\n",
    "secondary = values[thresh_idx:-1]\n",
    "print('\\nsec ', secondary)\n",
    "med = statistics.median(secondary)\n",
    "threshold = statistics.median([abs(x-med) for x in secondary])\n",
    "# threshold= secondary[0]\n",
    "\n",
    "print('\\nthresh ', threshold)\n",
    "\n",
    "row_col_dict = {}\n",
    "column_dict= {}\n",
    "row_dict = {}\n",
    "confidence_col = {}\n",
    "confidence_row = {}\n",
    "\n",
    "for column in table:\n",
    "    for index, row in table.iterrows():\n",
    "        if (str(row[column]) != 'nan'):\n",
    "            increment = 1 if (row[column] >= threshold) else 0\n",
    "            if (column in column_dict.keys()):\n",
    "                column_dict[column] += increment\n",
    "                confidence_col[column].append(row[column])\n",
    "            else:\n",
    "                column_dict[column] = increment\n",
    "                confidence_col[column] = [row[column]]\n",
    "\n",
    "            if (index in row_dict.keys()):\n",
    "                row_dict[index] += increment\n",
    "                confidence_row[index].append(row[column])\n",
    "            else:\n",
    "                row_dict[index] = increment\n",
    "                confidence_row[index] = [row[column]]\n",
    "\n",
    "            if index == column:\n",
    "                row_col_dict[index] = True\n",
    "            else:\n",
    "                if index not in row_col_dict.keys():\n",
    "                    row_col_dict[index] = False\n",
    "print('\\nrow col dict', row_col_dict)\n",
    "\n",
    "for key, value in confidence_col.items():\n",
    "    confidence_col[key] = statistics.mean(value)\n",
    "for key, value in confidence_row.items():\n",
    "    confidence_row[key] = statistics.mean(value)\n",
    "\n",
    "final_dict = {}\n",
    "for key in row_dict.keys():\n",
    "    if (key in column_dict.keys()):\n",
    "        row_dict[key] = (row_dict[key]+ column_dict[key]) //2\n",
    "        confidence_row[key] = (confidence_row[key] + confidence_col[key]) //2\n",
    "    final_dict[key] = row_dict[key] * confidence_row[key]\n",
    "print('\\nrow dict', row_dict)\n",
    "print()\n",
    "\n",
    "output = []\n",
    "final_rows =  {v:k for k, v in sorted(final_dict.items(), key=operator.itemgetter(1),reverse=True)}\n",
    "\n",
    "vals = list(final_rows.keys())\n",
    "        \n",
    "X = np.reshape(vals, (-1, 1))\n",
    "iso = IsolationForest()\n",
    "yhat = iso.fit_predict(X)\n",
    "print('\\nyhat ', yhat)\n",
    "\n",
    "\n",
    "medi = statistics.median(vals)\n",
    "for i in range(len(yhat)):\n",
    "        if (yhat[i] == -1) and (vals[i]> med):\n",
    "            output.append(final_rows[vals[i]])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{88.0: 'os_022',\n",
       " 18.0: 'docker_002',\n",
       " 14.746453446564528: 'db_003',\n",
       " 13.702658743312483: 'db_007',\n",
       " 13.686794342986808: 'db_009',\n",
       " 12.0: 'os_021',\n",
       " 9.0: 'docker_001',\n",
       " 6.0: 'docker_008'}"
      ]
     },
     "metadata": {},
     "execution_count": 685
    }
   ],
   "source": [
    "final_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[88.0,\n",
       " 18.0,\n",
       " 14.746453446564528,\n",
       " 13.702658743312483,\n",
       " 13.686794342986808,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 6.0]"
      ]
     },
     "metadata": {},
     "execution_count": 680
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1, -1,  1,  1,  1,  1,  1, -1])"
      ]
     },
     "metadata": {},
     "execution_count": 675
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}