{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from termcolor import colored\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from threading import Thread, Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESB_Analyzer():\n",
    "    def __init__(self, esb_data):\n",
    "        self.esb_data = esb_data\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        filename = 'birch_model_avgtime.sav'\n",
    "        self.avg_time_model = pickle.load(open(filename, 'rb'))\n",
    "        filename = 'birch_model_success.sav'\n",
    "        self.succee_rate_model = pickle.load(open(filename, 'rb'))\n",
    "        print('ESB models loaded')\n",
    "\n",
    "    def update_esb_data(self, esb_data):\n",
    "        self.esb_data = esb_data\n",
    "\n",
    "    def birch(self, values, ctype):  # values should be a list\n",
    "        X = np.reshape(values, (-1, 1))\n",
    "        brc = self.avg_time_model if ctype == \"time\" else self.succee_rate_model\n",
    "        return brc.predict(X)\n",
    "\n",
    "    def analyze_esb(self, esb_dict):\n",
    "        esb_tmp = self.esb_data.append(esb_dict, ignore_index=True)\n",
    "        values = esb_tmp['avg_time'].tolist()\n",
    "        # print(values)\n",
    "        birch_labels_time = self.birch(values,\"time\")\n",
    "        # birch_labels_rate = self.birch(self.esb_data['avg_time'])\n",
    "        for label in birch_labels_time:\n",
    "            if (label != 0):\n",
    "                print(\"Found esb_anomaly in avg_time\")\n",
    "                return True\n",
    "\n",
    "        values = esb_tmp['succee_rate'].tolist()\n",
    "        # print(values)\n",
    "        birch_labels_time = self.birch(values,\"rate\")\n",
    "        for label in birch_labels_time:\n",
    "            if (label != 0):\n",
    "                print(\"Found esb_anomaly in success rate\")\n",
    "                return True\n",
    "\n",
    "        self.update_esb_data(esb_tmp)\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "class RCA():\n",
    "    def __init__(self, trace_data, host_data, alpha=0.95, ub=0.02, take_minute_averages_of_trace_data=True, division_milliseconds=60000):\n",
    "        self.trace_data = trace_data\n",
    "        self.host_data = host_data\n",
    "        self.alpha = alpha\n",
    "        self.ub = ub\n",
    "        self.anomaly_chart = None\n",
    "        self.take_minute_averages_of_trace_data = take_minute_averages_of_trace_data\n",
    "        self.division_milliseconds = division_milliseconds\n",
    "\n",
    "    def run(self):\n",
    "        self.trace_processing()\n",
    "        \n",
    "        print('Running RCA on %d trace data rows and %d host data rows' %\n",
    "              (len(self.trace_data), len(self.host_data)))\n",
    "        overall_start_time = time.time()\n",
    "\n",
    "        print('Computing for anomaly score chart...')\n",
    "        self.hesd_trace_detection(alpha=self.alpha, ub=self.ub)\n",
    "        print('Score chart computed!')\n",
    "\n",
    "        self.local_initiate()\n",
    "        output = self.find_anomalous_rows()\n",
    "\n",
    "        print('The output to send to the server is: ' +\n",
    "              colored(str(output), 'magenta'))\n",
    "\n",
    "        print('RCA finished in ' + colored('%f', 'cyan') %\n",
    "              (time.time() - overall_start_time) + ' seconds.')\n",
    "        return output\n",
    "\n",
    "    def esd_test_statistics(self, x, hybrid=True):\n",
    "        \"\"\"\n",
    "        Compute the location and dispersion sample statistics used to carry out the ESD test.\n",
    "        \"\"\"\n",
    "        if hybrid:\n",
    "            location = pd.Series(x).median(skipna=True) # Median\n",
    "            dispersion = np.median(np.abs(x - np.median(x))) # Median Absolute Deviation\n",
    "        else:  \n",
    "            location = pd.Series(x).mean(skipna=True) # Mean\n",
    "            dispersion = pd.Series(x).std(skipna=True) # Standard Deviation\n",
    "            \n",
    "        return location, dispersion\n",
    "\n",
    "    def esd_test(self, x, alpha=0.95, ub=0.499, hybrid=True):\n",
    "        \"\"\"\n",
    "        Carries out the Extreme Studentized Deviate(ESD) test which can be used to detect one or more outliers present in the timeseries\n",
    "        \n",
    "        x      : List, array, or series containing the time series\n",
    "        freq   : Int that gives the number of periods per cycle (7 for week, 12 for monthly, etc)\n",
    "        alpha  : Confidence level in detecting outliers\n",
    "        ub     : Upper bound on the fraction of datapoints which can be labeled as outliers (<=0.499)\n",
    "        hybrid : Whether to use the robust statistics (median, median absolute error) or the non-robust versions (mean, standard deviation) to test for anomalies\n",
    "        \"\"\"\n",
    "        nobs = len(x)\n",
    "        if ub > 0.4999:\n",
    "            ub = 0.499\n",
    "        k = max(int(np.floor(ub * nobs)), 1) # Maximum number of anomalies. At least 1 anomaly must be tested.\n",
    "        #   res_tmp = ts_S_Md_decomposition(x)[\"residual\"] # Residuals from time series decomposition\n",
    "            \n",
    "        # Carry out the esd test k times  \n",
    "        res = np.ma.array(x, mask=False) # The \"ma\" structure allows masking of values to exclude the elements from any calculation\n",
    "        anomalies = [] # returns the indices of the found anomalies\n",
    "        for i in range(1, k+1):\n",
    "            location, dispersion = self.esd_test_statistics(res, hybrid) # Sample statistics\n",
    "            tmp = np.abs(res - location) / dispersion\n",
    "            idx = np.argmax(tmp) # Index of the test statistic\n",
    "            test_statistic = tmp[idx] \n",
    "            n = nobs - res.mask.sum() # sums  nonmasked values\n",
    "            critical_value = (n - i) * t.ppf(alpha, n - i - 1) / np.sqrt((n - i - 1 + np.power(t.ppf(alpha, n - i - 1), 2)) * (n - i - 1)) \n",
    "            if test_statistic > critical_value:\n",
    "                anomalies.append(test_statistic)\n",
    "            res.mask[idx] = True\n",
    "        if len(anomalies) == 0:\n",
    "            return 0\n",
    "        return np.mean(anomalies)\n",
    "    \n",
    "    def hesd_trace_detection(self, alpha=0.95, ub=0.02):\n",
    "        grouped_df = self.trace_data.groupby(['cmdb_id', 'serviceName'])[['startTime','actual_time']]\n",
    "\n",
    "        self.anomaly_chart = pd.DataFrame()\n",
    "        for (a, b), value in grouped_df:\n",
    "            value['time_group'] = value.startTime//self.division_milliseconds\n",
    "            value = value.groupby(['time_group'])['actual_time'].mean().reset_index()\n",
    "            result = self.esd_test(value['actual_time'].to_numpy(), alpha=alpha, ub=ub, hybrid=True)\n",
    "            self.anomaly_chart.loc[b,a] = result\n",
    "\n",
    "        self.anomaly_chart = self.anomaly_chart.sort_index()\n",
    "        print(self.anomaly_chart)\n",
    "        # print(self.anomaly_chart.to_dict())\n",
    "        return self.anomaly_chart\n",
    "    \n",
    "    def local_initiate(self):\n",
    "        self.dockers = ['docker_001', 'docker_002', 'docker_003', 'docker_004',\n",
    "                'docker_005', 'docker_006', 'docker_007', 'docker_008']\n",
    "        self.docker_hosts = ['os_017', 'os_018', 'os_019', 'os_020']\n",
    "\n",
    "        self.docker_kpi_names = ['container_cpu_used', None]\n",
    "        self.os_kpi_names = ['Sent_queue', 'Received_queue']\n",
    "        self.db_kpi_names = ['Proc_User_Used_Pct','Proc_Used_Pct','Sess_Connect','On_Off_State', 'tnsping_result_time']\n",
    "\n",
    "        self.docker_lookup_table = {}\n",
    "        for i in range(len(self.dockers)):\n",
    "            self.docker_lookup_table[self.dockers[i]] = self.docker_hosts[i % 4]\n",
    "\n",
    "\n",
    "    def find_anomalous_rows(self, min_threshold = 5):\n",
    "        table = self.anomaly_chart.copy()\n",
    "        threshold = max( 0.5 * table.stack().max(), min_threshold)\n",
    "        dodgy_rows = []\n",
    "        just_rows = []\n",
    "        row_col_dict = {}\n",
    "        for column in table:\n",
    "            v = 0\n",
    "            r = ''\n",
    "            for index, row in table.iterrows():\n",
    "                if (row[column] > threshold):\n",
    "                    if index == column:\n",
    "                        dodgy_rows.append([index, column, row[column]])\n",
    "                        just_rows.append(index)\n",
    "                        row_col_dict[index] = True\n",
    "                        break\n",
    "                    elif (row[column] > v):\n",
    "                        v = row[column]\n",
    "                        r = index\n",
    "            if r != '':\n",
    "                dodgy_rows.append([r, column, v])\n",
    "                just_rows.append(r)\n",
    "                if r not in row_col_dict.keys():\n",
    "                    row_col_dict[r] = False\n",
    "        output = self.localize(dodgy_rows, list(set(just_rows)), row_col_dict)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def find_anomalous_kpi(self, cmdb_id, row_col_same = False):\n",
    "        kpi_names = []\n",
    "        if 'os' in cmdb_id:\n",
    "            kpi_names = self.os_kpi_names\n",
    "        elif 'docker' in cmdb_id:\n",
    "            kpi_names = [self.docker_kpi_names[0]] if row_col_same else [self.docker_kpi_names[1]]\n",
    "        else:\n",
    "            kpi_names = self.db_kpi_names\n",
    "            host_data_subset = self.host_data.loc[(self.host_data.cmdb_id == cmdb_id) & (self.host_data.name.isin(kpi_names))]\n",
    "            results_dict = {}\n",
    "            for kpi, values in host_data_subset.groupby('name')['value']:\n",
    "                values = list(values)\n",
    "                score =  self.esd_test(np.array(values), 0.95, 0.1, True)\n",
    "                results_dict[kpi] = score\n",
    "            db_connection_limit = [results_dict['Proc_User_Used_Pct'],results_dict['Proc_Used_Pct'],results_dict['Sess_Connect']]\n",
    "            db_connection_limit_score = np.mean(db_connection_limit)\n",
    "            db_close = [results_dict['On_Off_State'],results_dict['tnsping_result_time']]\n",
    "            db_close_score = np.mean(db_close)\n",
    "            if db_connection_limit_score > db_close_score:\n",
    "                kpi_names = kpi_names[:3]\n",
    "            else:\n",
    "                kpi_names = kpi_names[3:]             \n",
    "\n",
    "        return kpi_names\n",
    "\n",
    "\n",
    "    def localize(self, dodgy_rows, just_rows, row_col_dict):\n",
    "        n = len(just_rows)\n",
    "        print('%d anomalies found' % n)\n",
    "        if n < 1:\n",
    "            return None\n",
    "        if n == 1:\n",
    "            KPIs = self.find_anomalous_kpi(just_rows[0], row_col_dict[list(row_col_dict.keys())[0]])\n",
    "            to_be_sent = []\n",
    "            for KPI in KPIs:\n",
    "                to_be_sent.append([just_rows[0], KPI])\n",
    "            return to_be_sent\n",
    "        if n == 2:\n",
    "            r0 = just_rows[0]\n",
    "            r1 = just_rows[1]\n",
    "            if ('os' in r0) and ('os' in r1):\n",
    "                KPIS = self.find_anomalous_kpi('os_001')\n",
    "                return [['os_001', KPIS[0]], ['os_001', KPIS[1]]]\n",
    "\n",
    "            if ('docker' in r0) and ('docker' in r1):\n",
    "                if self.docker_lookup_table[r0] == self.docker_lookup_table[r1]:\n",
    "                    KPIS = self.find_anomalous_kpi(self.docker_lookup_table[r0])\n",
    "                    to_be_sent = []\n",
    "                    for kpi in KPIS:\n",
    "                        to_be_sent.append([self.docker_lookup_table[r0], kpi])\n",
    "                    return to_be_sent\n",
    "\n",
    "            KPI0s = self.find_anomalous_kpi(r0, row_col_dict[r0])\n",
    "            KPI1s = self.find_anomalous_kpi(r1, row_col_dict[r1])\n",
    "            to_be_sent = []\n",
    "            for kpi in KPI0s:\n",
    "                to_be_sent.append([r0, kpi])\n",
    "            for kpi in KPI1s:\n",
    "                to_be_sent.append([r1, kpi])\n",
    "            return to_be_sent\n",
    "        if n > 2:\n",
    "            dodgy_rows.sort(key = lambda x: x[2], reverse = True)\n",
    "            just_rows = [x[0] for x in dodgy_rows]\n",
    "            just_rows = list(np.unique(just_rows))\n",
    "            row_col_dict = { just_rows[0]: row_col_dict[just_rows[0]], just_rows[1]: row_col_dict[just_rows[1]] } \n",
    "            return self.localize(dodgy_rows[:2], just_rows[:2], row_col_dict)\n",
    "\n",
    "\n",
    "    def update_trace_data(self, trace_data):\n",
    "        self.trace_data = trace_data\n",
    "\n",
    "    def update_host_data(self, host_data):\n",
    "        self.host_data = host_data\n",
    "\n",
    "    def trace_processing(self):\n",
    "        print(\"Started trace processing\")\n",
    "        p_time = time.time()\n",
    "        self.trace_data = self.trace_data[self.trace_data['callType'] != 'FlyRemote'].copy()\n",
    "        df1 = self.trace_data[self.trace_data['callType']=='RemoteProcess']\n",
    "        df1 = df1[['pid','cmdb_id']]\n",
    "        df1 = df1.set_index('pid')\n",
    "\n",
    "        csf_cmdb = df1.to_dict()\n",
    "        csf_cmdb = {str(key):str(values) for key, values in csf_cmdb['cmdb_id'].items()}\n",
    "\n",
    "        # for index, row in self.trace_data.iterrows():\n",
    "        #     if row['id'] in csf_cmdb:\n",
    "        #         self.trace_data.at[index, 'serviceName'] = csf_cmdb[row['id']]\n",
    "\n",
    "        elapse_time = {}\n",
    "        children = {}\n",
    "\n",
    "        def do_thing(row):\n",
    "            ## if change 297 and 298, gets different result??? loook into this plz\n",
    "            if row['id'] in csf_cmdb:\n",
    "                row['serviceName'] = csf_cmdb[row['id']]\n",
    "            if row['pid'] != 'None':\n",
    "                children[row['pid']] = children.get(row['pid'], [])\n",
    "                children[row['pid']].append(row['id'])\n",
    "            elapse_time[row['id']] = float(row['elapsedTime'])\n",
    "            return row\n",
    "        self.trace_data = self.trace_data.apply(do_thing, axis=1)\n",
    "\n",
    "\n",
    "        # for index, row in self.trace_data.iterrows():\n",
    "        #     if row['pid'] != 'None':\n",
    "        #         if row['pid'] in children.keys():\n",
    "        #             children[row['pid']].append(row['id'])\n",
    "        #         else:\n",
    "        #             children[row['pid']] = [row['id']]\n",
    "        #     elapse_time[row['id']] = float(row['elapsedTime'])\n",
    "        \n",
    "        # self.trace_data.apply(parse, axis = 1)\n",
    "\n",
    "        self.trace_data['actual_time'] = 0.0\n",
    "        # for index, row in self.trace_data.iterrows():\n",
    "        #     total_child = 0.0\n",
    "        #     if row['id'] not in children.keys():\n",
    "        #         self.trace_data.at[index, 'actual_time'] = row['elapsedTime']\n",
    "        #         continue\n",
    "        #     for child in children[row['id']]:\n",
    "        #         total_child += elapse_time[child]\n",
    "        #     self.trace_data.at[index, 'actual_time'] = row['elapsedTime'] - total_child\n",
    "\n",
    "        def get_actual_time(row):\n",
    "            total_child = 0.0\n",
    "            if row['id'] in children:\n",
    "                for child in children[row['id']]:\n",
    "                    total_child += elapse_time[child]\n",
    "            row['actual_time'] = row['elapsedTime'] - total_child\n",
    "            return row\n",
    "        \n",
    "        self.trace_data = self.trace_data.apply(get_actual_time, axis = 1)\n",
    "        \n",
    "        self.trace_data = self.trace_data[~(self.trace_data['serviceName'].str.contains('csf', na=True))]\n",
    "\n",
    "        print(\"Trace processed in \", time.time()-p_time, 'seconds')\n",
    "        print(self.trace_data)\n",
    "\n",
    "\n",
    "# Three topics are available: platform-index, business-index, trace.\n",
    "# Subscribe at least one of them.\n",
    "AVAILABLE_TOPICS = set(['platform-index', 'business-index', 'trace'])\n",
    "# CONSUMER = KafkaConsumer('platform-index', 'business-index', 'trace',\n",
    "#                          bootstrap_servers=['172.21.0.8', ],\n",
    "#                          auto_offset_reset='latest',\n",
    "#                          enable_auto_commit=False,\n",
    "#                          security_protocol='PLAINTEXT')\n",
    "\n",
    "\n",
    "class Trace():  # pylint: disable=invalid-name,too-many-instance-attributes,too-few-public-methods\n",
    "    '''Structure for traces'''\n",
    "\n",
    "    __slots__ = ['call_type', 'start_time', 'elapsed_time', 'success',\n",
    "                 'trace_id', 'id', 'pid', 'cmdb_id', 'service_name', 'ds_name']\n",
    "\n",
    "    def __new__(self, data):\n",
    "        self.trace = data\n",
    "        if self.trace['callType'] == 'JDBC' or self.trace['callType']=='LOCAL':\n",
    "            try:\n",
    "                self.trace['serviceName'] = data['dsName']\n",
    "            except:\n",
    "                print(data)\n",
    "                print('JDBC doesnt have dsName')\n",
    "        \n",
    "        elif self.trace['callType']=='RemoteProcess' or self.trace['callType']=='OSB':\n",
    "            self.trace['serviceName'] = data['cmdb_id']\n",
    "\n",
    "        if 'dsName' in self.trace:\n",
    "            self.trace.pop('dsName')\n",
    "\n",
    "        return self.trace\n",
    "\n",
    "\n",
    "def detection(timestamp):\n",
    "    global host_df, trace_df\n",
    "    print('Starting Anomaly Detection')\n",
    "    startTime = timestamp - 1200000  # one minute before anomaly\n",
    "\n",
    "    # print(len(trace_df), trace_df.head())\n",
    "    # print(len(host_df), host_df.head())\n",
    "    trace_df_temp = trace_df[(trace_df['startTime'] >= startTime) &\n",
    "                             (trace_df['startTime'] <= timestamp)]\n",
    "    host_df_temp = host_df[(host_df['timestamp'] >= startTime) &\n",
    "                           (host_df['timestamp'] <= timestamp)]\n",
    "    print(len(trace_df_temp), trace_df_temp.head())\n",
    "    print(len(host_df_temp), host_df_temp.head())\n",
    "\n",
    "    rca_temp = RCA(trace_data=trace_df_temp, host_data=host_df_temp)\n",
    "    results_to_send_off = rca_temp.run()\n",
    "\n",
    "    print('Anomaly Detection Done.')\n",
    "    if results_to_send_off is None:\n",
    "        # print('Nothing detected')\n",
    "        return False, rca_temp\n",
    "    # for a in anom_hosts:\n",
    "    #     item = a.split(':')[0]\n",
    "    #     if (item not in anoms):\n",
    "    #         anoms.append(item)\n",
    "    # print(results_to_send_off)\n",
    "    # submit(results_to_send_off)\n",
    "    return True, rca_temp\n",
    "\n",
    "\n",
    "def rcaprocess(esb_item, trace, host, timestamp, lock):\n",
    "    global host_df, trace_df, esb_anal, a_time\n",
    "    esb_anomaly = False\n",
    "\n",
    "    # print(trace)\n",
    "    trace_df = trace_df[(trace_df.startTime >= (timestamp-1260000))]\n",
    "    host_df = host_df[(host_df.timestamp >= (timestamp-1260000))]\n",
    "    \n",
    "    t = time.time()\n",
    "    t_df = pd.DataFrame(trace)\n",
    "    h_df = pd.DataFrame(host)\n",
    "\n",
    "    trace_df = pd.concat([trace_df, t_df], axis=0, ignore_index=True)\n",
    "    host_df = pd.concat([host_df, h_df], axis=0, ignore_index=True)\n",
    "\n",
    "    print('Time to add new data: ', (time.time()-t))\n",
    "\n",
    "    print(esb_anal.esb_data.tail(1))\n",
    "    print(host_df.tail(1))\n",
    "    print(trace_df.tail(1))\n",
    "\n",
    "    with lock:\n",
    "        esb_anomaly = esb_anal.analyze_esb(esb_item)\n",
    "        if (time.time() - a_time) >= 600 and esb_anomaly:\n",
    "            tmp_time = time.time()\n",
    "            print(\"oops\")\n",
    "            # detection(timestamp)\n",
    "            result = detection(timestamp)\n",
    "            print('Anomaly at: ', timestamp)\n",
    "            if result:\n",
    "                a_time = tmp_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Unnamed: 0           itemid                      name     bomc_id  \\\n",
      "0          173549         63309180  Incoming_network_traffic  ZJ-001-069   \n",
      "1          521522  999999998651100             MEM_real_util  ZJ-002-053   \n",
      "2          521523  999999998651280              CPU_free_pct  ZJ-002-056   \n",
      "3          521524  999999998650680              CPU_free_pct  ZJ-002-056   \n",
      "4          521525  999999998650980              CPU_free_pct  ZJ-002-056   \n",
      "...           ...              ...                       ...         ...   \n",
      "41601      194578         65439900              Disk_io_util  ZJ-001-125   \n",
      "41602      194579         63308940  Outgoing_network_traffic  ZJ-001-072   \n",
      "41603      194582  999999996431220             FS_used_space  ZJ-001-108   \n",
      "41604      194580  999999996430740                Send_total  ZJ-001-074   \n",
      "41605      533670  999999998651100             MEM_real_util  ZJ-002-053   \n",
      "\n",
      "           timestamp      value cmdb_id  \n",
      "0      1590348420000   0.000000  os_015  \n",
      "1      1590348420000  81.820000  db_007  \n",
      "2      1590348420000  97.418928  db_008  \n",
      "3      1590348420000  98.558964  db_001  \n",
      "4      1590348420000  99.001416  db_003  \n",
      "...              ...        ...     ...  \n",
      "41601  1590349920000   1.000000  os_012  \n",
      "41602  1590349920000   0.032788  os_022  \n",
      "41603  1590349920000   5.833800  os_014  \n",
      "41604  1590349920000   0.000479  os_006  \n",
      "41605  1590349920000  81.860000  db_007  \n",
      "\n",
      "[41606 rows x 7 columns]\n",
      "Starting Anomaly Detection\n",
      "439754        Unnamed: 0 callType      startTime  elapsedTime  success  \\\n",
      "47113    10115781      OSB  1590348540022       1013.0     True   \n",
      "47114    10115782      OSB  1590348540022        660.0     True   \n",
      "47115    10115783      OSB  1590348540022        458.0     True   \n",
      "47116    10115786      OSB  1590348540023       1758.0     True   \n",
      "47117    10115787      OSB  1590348540023        657.0     True   \n",
      "\n",
      "                    traceId                    id   pid cmdb_id serviceName  \\\n",
      "47113  3a1712e1a061746786d5  3a17108dfc71746786d5  None  os_022      os_022   \n",
      "47114  3a1713172449046786d5  3a171703d859046786d5  None  os_021      os_021   \n",
      "47115  3a171367a121036786d5  3a171ea48531036786d5  None  os_021      os_021   \n",
      "47116  3a1711198a47077786d5  3a171a583257077786d5  None  os_021      os_021   \n",
      "47117  3a171ca9f169047786d5  3a171d406a79047786d5  None  os_021      os_021   \n",
      "\n",
      "      dsName  \n",
      "47113    NaN  \n",
      "47114    NaN  \n",
      "47115    NaN  \n",
      "47116    NaN  \n",
      "47117    NaN  \n",
      "33076       Unnamed: 0           itemid                      name     bomc_id  \\\n",
      "3605      522759  999999998651100             MEM_real_util  ZJ-002-053   \n",
      "3606      522760  999999998651280              CPU_free_pct  ZJ-002-056   \n",
      "3607      522761  999999998650680              CPU_free_pct  ZJ-002-056   \n",
      "3608      522762  999999998650980              CPU_free_pct  ZJ-002-056   \n",
      "3609      175241         63309180  Incoming_network_traffic  ZJ-001-069   \n",
      "\n",
      "          timestamp      value cmdb_id  \n",
      "3605  1590348540000  81.820000  db_007  \n",
      "3606  1590348540000  96.909711  db_008  \n",
      "3607  1590348540000  98.734484  db_001  \n",
      "3608  1590348540000  99.006747  db_003  \n",
      "3609  1590348540000   0.000000  os_015  \n",
      "Started trace processing\n",
      "Trace processed in  43.08410429954529 seconds\n",
      "        Unnamed: 0 callType      startTime  elapsedTime  success  \\\n",
      "47113     10115781      OSB  1590348540022       1013.0     True   \n",
      "47114     10115782      OSB  1590348540022        660.0     True   \n",
      "47115     10115783      OSB  1590348540022        458.0     True   \n",
      "47116     10115786      OSB  1590348540023       1758.0     True   \n",
      "47117     10115787      OSB  1590348540023        657.0     True   \n",
      "...            ...      ...            ...          ...      ...   \n",
      "486862     8726340    LOCAL  1590349739990          7.0     True   \n",
      "486863     4830960     JDBC  1590349739990          3.0     True   \n",
      "486864     4830961     JDBC  1590349739994          3.0     True   \n",
      "486865     8726341    LOCAL  1590349739998          5.0     True   \n",
      "486866     4830962     JDBC  1590349739999          3.0     True   \n",
      "\n",
      "                     traceId                    id                   pid  \\\n",
      "47113   3a1712e1a061746786d5  3a17108dfc71746786d5                  None   \n",
      "47114   3a1713172449046786d5  3a171703d859046786d5                  None   \n",
      "47115   3a171367a121036786d5  3a171ea48531036786d5                  None   \n",
      "47116   3a1711198a47077786d5  3a171a583257077786d5                  None   \n",
      "47117   3a171ca9f169047786d5  3a171d406a79047786d5                  None   \n",
      "...                      ...                   ...                   ...   \n",
      "486862  3a1718a3dd65734ddaf6  3a171c76b872896d7bf6  3a1717a9bc32892d7bf6   \n",
      "486863  3a1718a3dd65734ddaf6  3a171d28a203896d7bf6  3a171c76b872896d7bf6   \n",
      "486864  3a1718a3dd65734ddaf6  3a171290283389ad7bf6  3a171c76b872896d7bf6   \n",
      "486865  3a1718a3dd65734ddaf6  3a171c4c085389ed7bf6  3a1717a9bc32892d7bf6   \n",
      "486866  3a1718a3dd65734ddaf6  3a171020a08389fd7bf6  3a171c4c085389ed7bf6   \n",
      "\n",
      "           cmdb_id serviceName  dsName  actual_time  \n",
      "47113       os_022      os_022     NaN         24.0  \n",
      "47114       os_021      os_021     NaN         22.0  \n",
      "47115       os_021      os_021     NaN         29.0  \n",
      "47116       os_021      os_021     NaN       1314.0  \n",
      "47117       os_021      os_021     NaN         22.0  \n",
      "...            ...         ...     ...          ...  \n",
      "486862  docker_006      db_003  db_003          1.0  \n",
      "486863  docker_006      db_003  db_003          3.0  \n",
      "486864  docker_006      db_003  db_003          3.0  \n",
      "486865  docker_006      db_003  db_003          2.0  \n",
      "486866  docker_006      db_003  db_003          3.0  \n",
      "\n",
      "[432170 rows x 12 columns]\n",
      "Running RCA on 432170 trace data rows and 33076 host data rows\n",
      "Computing for anomaly score chart...\n",
      "            docker_001  docker_002  docker_003  docker_004  docker_005  \\\n",
      "db_003             NaN         NaN         NaN         NaN  943.069231   \n",
      "db_007       20.180509    0.000000    4.924854    3.053743         NaN   \n",
      "db_009       97.561687    2.121139    2.299331    1.936483         NaN   \n",
      "docker_001   39.495527         NaN         NaN         NaN         NaN   \n",
      "docker_002         NaN    4.322049         NaN         NaN         NaN   \n",
      "docker_003         NaN         NaN    4.427620         NaN         NaN   \n",
      "docker_004         NaN         NaN         NaN    3.693145         NaN   \n",
      "docker_005         NaN         NaN  195.309861  297.450219  554.813409   \n",
      "docker_006         NaN         NaN    2.562494    2.941249         NaN   \n",
      "docker_007  172.780776    3.724662         NaN         NaN         NaN   \n",
      "docker_008  153.225745    3.359309         NaN         NaN         NaN   \n",
      "os_021             NaN         NaN         NaN         NaN         NaN   \n",
      "os_022             NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "            docker_006  docker_007  docker_008     os_021      os_022  \n",
      "db_003       33.152546    8.703037    2.669991        NaN         NaN  \n",
      "db_007             NaN         NaN         NaN        NaN         NaN  \n",
      "db_009             NaN         NaN         NaN        NaN         NaN  \n",
      "docker_001         NaN         NaN         NaN        NaN  143.004575  \n",
      "docker_002         NaN         NaN         NaN        NaN   13.732843  \n",
      "docker_003         NaN         NaN         NaN   4.327116         NaN  \n",
      "docker_004         NaN         NaN         NaN   5.855458         NaN  \n",
      "docker_005         NaN         NaN         NaN        NaN         NaN  \n",
      "docker_006   16.305209         NaN         NaN        NaN         NaN  \n",
      "docker_007         NaN    3.433384         NaN        NaN         NaN  \n",
      "docker_008         NaN         NaN    4.822066        NaN         NaN  \n",
      "os_021             NaN         NaN         NaN  10.582965         NaN  \n",
      "os_022             NaN         NaN         NaN        NaN   18.704349  \n",
      "Score chart computed!\n",
      "2 anomalies found\n",
      "The output to send to the server is: \u001b[35m[['db_003', 'Proc_User_Used_Pct'], ['db_003', 'Proc_Used_Pct'], ['db_003', 'Sess_Connect'], ['docker_005', 'container_cpu_used']]\u001b[0m\n",
      "RCA finished in \u001b[36m0.197666\u001b[0m seconds.\n",
      "Anomaly Detection Done.\n"
     ]
    }
   ],
   "source": [
    "global host_df, trace_df\n",
    "\n",
    "path = 'training_data/'\n",
    "trace_df = pd.read_csv(path + 'trace_data_os17_new.csv')\n",
    "# trace_df = trace_df.drop(['actual_time','path'], axis=1)\n",
    "# trace_df = trace_df.drop(['path'], axis=1)\n",
    "trace_df = trace_df.sort_values(by=['startTime'], ignore_index=True)\n",
    "# trace = trace[trace.startTime < trace.startTime[0]+1260000]\n",
    "\n",
    "host_df = pd.read_csv(path + 'kpi_data_os17_new.csv')\n",
    "host_df = host_df.sort_values(by=['timestamp'], ignore_index=True)\n",
    "\n",
    "# print(trace_df)\n",
    "print(host_df)\n",
    "timestamp = int(host_df['timestamp'].iloc[-1]-180000)\n",
    "# print(timestamp)\n",
    "# trace_df = trace_df[(trace_df.startTime >= (timestamp-1260000))]\n",
    "# print(host_df)\n",
    "boo, rca = detection(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            docker_001  docker_002  docker_003  docker_004  docker_005  \\\n",
       "db_003             NaN         NaN         NaN         NaN    4.214410   \n",
       "db_007        2.981341    3.329525    4.505819    4.367334         NaN   \n",
       "db_009        3.020953    1.941662    4.294422    2.561447         NaN   \n",
       "docker_001    4.853139         NaN         NaN         NaN         NaN   \n",
       "docker_002         NaN    2.221480         NaN         NaN         NaN   \n",
       "docker_003         NaN         NaN    3.429683         NaN         NaN   \n",
       "docker_004         NaN         NaN         NaN    2.534765         NaN   \n",
       "docker_005    3.551310    3.423252         NaN         NaN    3.686402   \n",
       "docker_006    6.032548    2.921921         NaN         NaN         NaN   \n",
       "docker_007         NaN         NaN    4.127451    3.463051         NaN   \n",
       "docker_008         NaN         NaN    4.260660    2.814533         NaN   \n",
       "os_021             NaN         NaN         NaN         NaN         NaN   \n",
       "os_022             NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "            docker_006  docker_007  docker_008    os_021     os_022  \n",
       "db_003        4.970412    7.587338    9.802947       NaN        NaN  \n",
       "db_007             NaN         NaN         NaN       NaN        NaN  \n",
       "db_009             NaN         NaN         NaN       NaN        NaN  \n",
       "docker_001         NaN         NaN         NaN       NaN   7.398257  \n",
       "docker_002         NaN         NaN         NaN       NaN   9.576413  \n",
       "docker_003         NaN         NaN         NaN  4.007030        NaN  \n",
       "docker_004         NaN         NaN         NaN  4.752495        NaN  \n",
       "docker_005         NaN         NaN         NaN       NaN        NaN  \n",
       "docker_006    7.576686         NaN         NaN       NaN        NaN  \n",
       "docker_007         NaN    6.948016         NaN       NaN        NaN  \n",
       "docker_008         NaN         NaN    2.469204       NaN        NaN  \n",
       "os_021             NaN         NaN         NaN  4.373459        NaN  \n",
       "os_022             NaN         NaN         NaN       NaN  92.449264  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>docker_001</th>\n      <th>docker_002</th>\n      <th>docker_003</th>\n      <th>docker_004</th>\n      <th>docker_005</th>\n      <th>docker_006</th>\n      <th>docker_007</th>\n      <th>docker_008</th>\n      <th>os_021</th>\n      <th>os_022</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>db_003</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.214410</td>\n      <td>4.970412</td>\n      <td>7.587338</td>\n      <td>9.802947</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>db_007</th>\n      <td>2.981341</td>\n      <td>3.329525</td>\n      <td>4.505819</td>\n      <td>4.367334</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>db_009</th>\n      <td>3.020953</td>\n      <td>1.941662</td>\n      <td>4.294422</td>\n      <td>2.561447</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_001</th>\n      <td>4.853139</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.398257</td>\n    </tr>\n    <tr>\n      <th>docker_002</th>\n      <td>NaN</td>\n      <td>2.221480</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.576413</td>\n    </tr>\n    <tr>\n      <th>docker_003</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.429683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.007030</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_004</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.534765</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.752495</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_005</th>\n      <td>3.551310</td>\n      <td>3.423252</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.686402</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_006</th>\n      <td>6.032548</td>\n      <td>2.921921</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.576686</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_007</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.127451</td>\n      <td>3.463051</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.948016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>docker_008</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.260660</td>\n      <td>2.814533</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.469204</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>os_021</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.373459</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>os_022</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>92.449264</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 587
    }
   ],
   "source": [
    "table = pd.DataFrame({'docker_001': {'db_003': np.nan, 'db_007': 2.9813413195424343, 'db_009': 3.0209533010263447, 'docker_001': 4.853138514249491, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': 3.551310136788956, 'docker_006': 6.0325483894159975, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_002': {'db_003': np.nan, 'db_007': 3.3295248353546856, 'db_009': 1.941661540366842, 'docker_001': np.nan, 'docker_002': 2.22147986919222, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': 3.4232516837614972, 'docker_006': 2.921920937021125, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_003': {'db_003': np.nan, 'db_007': 4.505818517747893, 'db_009': 4.2944222984892315, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': 3.4296830945060464, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': 4.127451447424727, 'docker_008': 4.260659945792008, 'os_021': np.nan, 'os_022': np.nan}, 'docker_004': {'db_003': np.nan, 'db_007': 4.367334230877272, 'db_009': 2.5614473718663886, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': 2.5347650585408923, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': 3.463050917628577, 'docker_008': 2.8145334777748636, 'os_021': np.nan, 'os_022': np.nan}, 'docker_005': {'db_003': 4.21441011069078, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': 3.6864016848501717, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_006': {'db_003': 4.970411934042088, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': 7.576686437915655, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_007': {'db_003': 7.587337997134692, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': 6.948016248094616, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': np.nan}, 'docker_008': {'db_003': 9.80294723945619, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': 2.4692037430959335, 'os_021': np.nan, 'os_022': np.nan}, 'os_021': {'db_003': np.nan, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': np.nan, 'docker_002': np.nan, 'docker_003': 4.007029991190557, 'docker_004': 4.752495280194077, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': 4.3734588741685645, 'os_022': np.nan}, 'os_022': {'db_003': np.nan, 'db_007': np.nan, 'db_009': np.nan, 'docker_001': 7.398256781684731, 'docker_002': 9.576412954443716, 'docker_003': np.nan, 'docker_004': np.nan, 'docker_005': np.nan, 'docker_006': np.nan, 'docker_007': np.nan, 'docker_008': np.nan, 'os_021': np.nan, 'os_022': 92.44926368399037}}\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['os_022']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['os_022', 'os_022', 92.44926368399037]]"
      ]
     },
     "metadata": {},
     "execution_count": 588
    }
   ],
   "source": [
    "threshold = max( 0.5 * table.stack().max(), 5)\n",
    "dodgy_rows = []\n",
    "just_rows = []\n",
    "row_col_dict = {}\n",
    "for column in table:\n",
    "    v = 0\n",
    "    r = ''\n",
    "    for index, row in table.iterrows():\n",
    "        if (row[column] > threshold):\n",
    "            if index == column:\n",
    "                dodgy_rows.append([index, column, row[column]])\n",
    "                just_rows.append(index)\n",
    "                row_col_dict[index] = True\n",
    "                break\n",
    "            elif (row[column] > v):\n",
    "                v = row[column]\n",
    "                r = index\n",
    "    if r != '':\n",
    "        dodgy_rows.append([r, column, v])\n",
    "        just_rows.append(r)\n",
    "        if r not in row_col_dict.keys():\n",
    "            row_col_dict[r] = False\n",
    "# output = self.localize(dodgy_rows, list(set(just_rows)), row_col_dict)\n",
    "print(list(set(just_rows)))\n",
    "dodgy_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nyhat  [-1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  1  1 -1 -1 -1 -1 -1 -1 -1 -1]\n\nsec  [6.0325483894159975, 6.948016248094616, 7.398256781684731, 7.576686437915655, 7.587337997134692, 9.576412954443716, 9.80294723945619]\n\nthresh  6.661218579237037\n\nrow col dict {'db_007': False, 'db_009': False, 'docker_001': True, 'docker_005': True, 'docker_006': True, 'docker_002': True, 'docker_003': True, 'docker_007': True, 'docker_008': True, 'docker_004': True, 'db_003': False, 'os_021': True, 'os_022': True}\n\nrow dict {'db_007': 4.367334230877272, 'db_009': 3.0129190866191093, 'docker_001': 6.0, 'docker_005': 3.0, 'docker_006': 6.0, 'docker_002': 5.0, 'docker_003': 4.0, 'docker_007': 7.0, 'docker_008': 5.0, 'docker_004': 3.0, 'db_003': 9.80294723945619, 'os_021': 4.0, 'os_022': 71.0}\n\n\nyhat  [-1 -1  1  1  1  1  1  1  1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['os_022', 'db_003']"
      ]
     },
     "metadata": {},
     "execution_count": 589
    }
   ],
   "source": [
    "import operator\n",
    "values = sorted(table.stack().tolist())\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# identify outliers in the training dataset\n",
    "X = np.reshape(values, (-1, 1))\n",
    "iso = IsolationForest()\n",
    "yhat = iso.fit_predict(X)\n",
    "print('\\nyhat ', yhat)\n",
    "\n",
    "import statistics\n",
    "\n",
    "thresh_idx = 0\n",
    "seen_normal = False\n",
    "for i in range(len(yhat)):\n",
    "    if (yhat[i] == 1):\n",
    "        seen_normal = True\n",
    "    elif (yhat[i] == -1) and seen_normal:\n",
    "        thresh_idx = i\n",
    "        break\n",
    "\n",
    "\n",
    "secondary = values[thresh_idx:-1]\n",
    "print('\\nsec ', secondary)\n",
    "med = statistics.median(secondary)\n",
    "threshold = statistics.median([abs(x-med) for x in secondary])+secondary[0]\n",
    "# threshold= secondary[0]\n",
    "\n",
    "print('\\nthresh ', threshold)\n",
    "\n",
    "row_col_dict = {}\n",
    "column_dict= {}\n",
    "row_dict = {}\n",
    "\n",
    "for column in table:\n",
    "    for index, row in table.iterrows():\n",
    "       if (str(row[column]) != 'nan'):\n",
    "           if (column in column_dict.keys()):\n",
    "               column_dict[column]= (column_dict[column]+ row[column])/2\n",
    "           else:\n",
    "               column_dict[column]= row[column]\n",
    "\n",
    "           if (index in row_dict.keys()):\n",
    "               row_dict[index]= (column_dict[column]+ row[column])/2\n",
    "           else:\n",
    "               row_dict[index]=row[column]\n",
    "\n",
    "           if index == column:\n",
    "               row_col_dict[index] = True\n",
    "           else:\n",
    "               if index not in row_col_dict.keys():\n",
    "                   row_col_dict[index] = False\n",
    "\n",
    "print('\\nrow col dict', row_col_dict)\n",
    "\n",
    "for key in row_dict.keys():\n",
    "    if (key in column_dict.keys()):\n",
    "        row_dict[key] = (row_dict[key]+ column_dict[key]) //2\n",
    "print('\\nrow dict', row_dict)\n",
    "print()\n",
    "\n",
    "just_rows =  {v:k for k, v in sorted(row_dict.items(), key=operator.itemgetter(1),reverse=True)}\n",
    "\n",
    "row_vals = list(just_rows.keys())\n",
    "X = np.reshape(row_vals, (-1, 1))\n",
    "iso = IsolationForest()\n",
    "yhat = iso.fit_predict(X)\n",
    "print('\\nyhat ', yhat)\n",
    "\n",
    "output = []\n",
    "for i in range(len(yhat)):\n",
    "    if (yhat[i] == -1):\n",
    "        output.append(just_rows[row_vals[i]])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['docker_002']"
      ]
     },
     "metadata": {},
     "execution_count": 586
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}